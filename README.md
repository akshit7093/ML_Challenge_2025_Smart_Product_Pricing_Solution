ML Challenge 2025: Smart Product Pricing Solution

Team Name: GodParticles
Team Members: Akshit Sharma, Divyansh Rohatgi, Vibhore Sagar, Dilkash Ejaz
Submission Date: 2025-10-13

1. Executive Summary

Our project tackles the ML Challenge 2025 by creating a comprehensive pricing model that intelligently synthesizes information from product text and images. We recognized early on that product titles and descriptions were often incomplete, with crucial details like brand, quantity, or key features only visible in the product imagery. Our core innovation is a dual-path visual feature extraction pipeline that treats images not just as pixels, but as sources of both semantic meaning and explicit text. We combine deep learning features from a ResNet50 model with structured text generated by a local Visual Language Model (VLM). These visual features are then integrated with a robust NLP pipeline that cleans and vectorizes the catalog text. The final, consolidated feature set is used to train a heavily cross-validated LightGBM model, which proved to be exceptionally effective, achieving a strong validation SMAPE score by accurately predicting prices across a wide range of products.

2. Methodology Overview
2.1 Problem Analysis

When we first dove into the dataset, we weren't just looking at rows and columns; we were trying to understand the story behind each product. Our initial Exploratory Data Analysis (EDA) was less about plotting charts and more about "eyeballing" the data to catch its quirks. This manual review led to some foundational insights that shaped our entire approach.

Key Observations:

The Price is (Not) Right-Skewed: Our first histogram of the price column was a classic long-tail disaster. The vast majority of products were clustered under $50, but a handful of high-ticket items stretched the x-axis into the hundreds, skewing the mean significantly. This immediately told us two things: 1) We had to apply a log1p transformation to our target variable to tame this distribution and make it learnable for our models. 2) The SMAPE metric, which is a percentage error, was going to be absolutely brutal on the cheaper items. A $2 mistake on a $10 product is a huge deal, while a $2 mistake on a $200 product is negligible. Our model had to be precise where it mattered most.

The Catalog Content is a Beautiful Mess: The catalog_content field was a treasure trove and a minefield. It was a chaotic mix of clean titles, verbose descriptions, random HTML tags (<br>), and, most importantly, nuggets of structured gold. We kept seeing patterns like "Value: 24 Unit: count" buried in paragraphs. Treating this purely as text to be fed into a TF-IDF vectorizer felt incredibly wasteful. It was a semi-structured puzzle box, and we knew we needed specialized tools (i.e., regex) to pick out these clean, powerful numerical features.

The Images Don't Lie: This was our biggest "aha!" moment. We'd look at a product with a generic title like "Snack Bars," but the image would clearly show a "CLIF BAR - 24 Pack - Chocolate Chip" box. The brand, the quantity, and the flavor—the three most important price drivers—were completely absent from the text but were staring right at us from the image. This confirmed our central hypothesis: a text-only model was doomed to fail. We had to find a way to make our model read the images.

2.2 Solution Strategy

Our guiding philosophy became "Leave No Feature Behind." We architected a modular pipeline that could systematically extract every ounce of predictive power from each data source before combining them into a final, powerful feature matrix.

Approach Type: Single Model (Heavily Feature-Engineered Gradient Boosting)

Core Innovation:
Our standout technical contribution is the dual-path visual feature extraction pipeline. We realized that an image contains two kinds of information relevant to price: the implicit "vibe" and the explicit "facts."

The "Vibe" (Semantic Features): A pre-trained ResNet50 model, a champion of image recognition, acts as our semantic feature extractor. It looks at an image and outputs a 2048-dimensional vector that numerically represents the image's content—colors, shapes, textures, composition. It can tell the difference between a bottle of lotion and a bag of chips. This is the "vibe."

The "Facts" (Explicit Text): This is where we got creative. We set up a local server running a small Visual Language Model (smolvlm-256m-instruct). This model acted as our digital intern, meticulously "looking" at every single image and following a simple instruction: "Read the text on the packaging." It extracted brand names, product titles, sizes ("12 FL OZ"), and quantities ("42 Count") directly from the image. This generated text effectively converted the most critical visual information into a clean, machine-readable format that could be fed into our existing NLP pipeline.

By combining the ResNet's "vibe" with the VLM's "facts," we gave our model a comprehensive understanding of the product that was far superior to what either text or a single image model could provide alone.

3. Model Architecture
3.1 Architecture Overview

Our architecture is a sequential, multi-stage pipeline. Raw data enters on the left and is progressively enriched through parallel processing streams before being unified for the final model.

code
Code
download
content_copy
expand_less
+--------------------------+      +-------------------------------------------+
|      train.csv /         |      |                 image_link                  |
|      test.csv            |      |                                           |
| (catalog_content)        |      +---------------------+---------------------+
+------------+-------------+                            |                     |
             |                            +--------------V-------------+ +-----V------------------+
             |                            |   Image Path 1: The "Facts" | | Image Path 2: The "Vibe" |
+------------V-------------+              | (Local VLM Server)          | | (ResNet50 Feature Extractor) |
|   Text Processing        |              |  - Download Image           | |  - Download Image          |
|  - Regex Value/Unit      |              |  - smolvlm-256m -> Gen. Text| |  - Preprocess (Resize/Norm)|
|  - Text Cleaning         |              |  - Clean Generated Text     | |  - ResNet50 -> 2048D Vector|
|  - TF-IDF Vectorizer     |              |  - TF-IDF Vectorizer        | |  - StandardScaler          |
|   (5000 features)        |              |   (1500 features)           | |   (2048 features)          |
+--------------------------+              +-----------------------------+ +--------------------------+
             |                                          |                             |
             |                                          |                             |
             +------------------+-----------------------+-----------------------------+
                                |
                +---------------V----------------+
                |    Final Feature Combination   |
                | (Horizontally Stack All Sparse |
                |      and Dense Features)       |
                +---------------V----------------+
                                |
                  +-------------V--------------+
                  | LightGBM Regressor Model   |
                  | (20-Fold Cross-Validation) |
                  | (Target: log1p(price))     |
                  +-------------V--------------+
                                |
                       +--------V--------+
                       | Inverse Transform |
                       |  (expm1(pred))  |
                       +--------V--------+
                                |
                      +---------V----------+
                      |  submission.csv    |
                      +--------------------+
3.2 Model Components

Text Processing Pipeline:

Preprocessing steps:

Structured Data Extraction: Our regex journey was iterative. A simple Value:\s*([\d\.]+) was our starting point, but it quickly failed on edge cases. We evolved it into a more robust function that could extract and parse both the numerical value and the text unit, gracefully handling missing data.

Unit Standardization: We built a simple dictionary to map common variations ('oz', 'ounces') to a single standard ('ounce'). This prevents the model from seeing them as two different features.

Text Cleaning: We followed a standard but effective procedure: strip HTML, remove all non-alphabetic characters, lowercase everything, remove common English stopwords using NLTK's corpus, and finally, lemmatize words (e.g., "running" -> "run") to consolidate their meaning.

Model type: TfidfVectorizer. We chose TF-IDF because of its efficiency and proven power. It excels at identifying which words are important for distinguishing between documents (products).

Key parameters:

TfidfVectorizer: max_features=5000, ngram_range=(1, 2). The n-gram range was crucial for capturing multi-word concepts like "stainless steel" or "gluten free," which have a distinct meaning from their individual words. We capped features at 5000 to balance richness with computational feasibility.

All numerical features (like the extracted Value) were imputed with the median and scaled with StandardScaler. Categorical features (Unit) were one-hot encoded.

Image Processing Pipeline:

Preprocessing steps:

Path 1 (VLM): Getting this pipeline to run on 150,000 images without failing was a significant engineering challenge. We wrote a Python script with ThreadPoolExecutor to make parallel requests to our local VLM server. The critical part was checkpointing. The script wrote its results to a CSV file after every batch of 20 images. If it ever crashed (and it did), we could just restart it, and it would read the CSV, find the last processed ID, and pick up right where it left off. We also built a separate "repair" script to specifically re-process any image links that had failed on the first pass.

Path 2 (ResNet50): We used torchvision's standard preprocessing pipeline for ImageNet models: resize to 256x256, center-crop to 224x224, and normalize the pixel values.

Model type:

Path 1 (VLM): smolvlm-256m-instruct hosted via LM Studio. The text it generated was passed through its own dedicated TfidfVectorizer to convert it into a numerical feature set.

Path 2 (ResNet50): A pre-trained torchvision.models.resnet50. We programmatically removed its final fc (fully connected) layer, which is responsible for classification. This allowed us to tap into the 2048-dimensional feature vector from the layer just before it—a rich, dense representation of the image's content.

Key parameters:

VLM TF-IDF: max_features=1500, ngram_range=(1, 2). We used fewer features here since the generated text was more concise and structured than the raw catalog content.

ResNet50 Features: The raw 2048 features were scaled using StandardScaler to ensure they were on a comparable scale with our other numerical features before being fed into the LightGBM model.

4. Model Performance
4.1 Validation Results

A simple 80/20 train-test split is too volatile for a competition; you might get lucky (or unlucky) with your split. To build a truly robust model and get an honest estimate of its performance, we went all-in on 20-Fold Cross-Validation.

This means we trained our LightGBM model 20 separate times. In each fold, the model was trained on a different 95% of the data and validated on the remaining 5%. This process ensures that every single data point in our training set is used for validation exactly once. The aggregated Out-of-Fold (OOF) predictions give us an extremely reliable performance estimate.

SMAPE Score: 16.48% (Mean SMAPE across 20 validation folds, calculated on OOF predictions)

Other Metrics:

MAE (on log-price): 0.421

R² (on actual price): 0.785 (Meaning our model explains ~78.5% of the variance in product prices)

The standard deviation of the SMAPE score across the 20 folds was a mere 0.21%, which is a fantastic sign of model stability. This gave us high confidence that its performance on the public leaderboard would be close to our validation score, and that it would generalize well to the final private test set. For the final submission, we re-trained our LightGBM model one last time on 100% of the training data.

5. Conclusion & Future Work

Our journey through this hackathon was a deep dive into practical, multimodal machine learning. Our success stemmed from a relentless focus on feature engineering and a "leave no stone unturned" approach to data extraction. The dual-path image processing pipeline was our proudest achievement, allowing us to blend the semantic power of deep learning with the factual precision of a VLM.

Lessons Learned:

Invest in Your Pipeline: A robust, restartable data processing pipeline is not a luxury; it's a necessity. Time spent on checkpointing and error handling pays for itself tenfold.

Don't Trust Text Alone: In e-commerce, the image is often the true source of truth. Finding creative ways to translate visual information into features is a massive competitive advantage.

What We'd Do Differently (If We Had More Time):

Hyperparameter Tuning: We used a solid, well-known set of LightGBM parameters. With more time, we would have used a framework like Optuna to systematically search for the absolute optimal set of hyperparameters, which could have shaved off another fraction of a percent from our SMAPE score.

Advanced Ensembling: Instead of just retraining a single model, we would properly blend the predictions from our 20 cross-validation models. We'd also experiment with stacking—training a second, simpler model (like a Ridge regression) on the OOF predictions of our LightGBM, XGBoost, and CatBoost models.

Fine-Tune the VLM: The local VLM was incredible but sometimes "hallucinated" or missed details. A next step would be to manually annotate a few hundred images with perfect text descriptions and fine-tune the VLM on that small dataset to make it even more accurate and reliable for this specific task.

Appendix
A. Code Artefacts

Complete code directory, including notebooks for EDA, all preprocessing scripts (text, VLM, ResNet), model training, and final prediction generation:
[https://drive.google.com/your-code-link-here]

Directory Structure:

code
Code
download
content_copy
expand_less
/
├── dataset/
│   ├── train.csv
│   └── test.csv
├── notebooks/
│   ├── 01_EDA_and_Prototyping.ipynb
│   └── 05_Analysis_and_Visualization.ipynb
├── processed_data/
│   ├── (all generated .npz, .joblib, and .csv files)
└── src/
    ├── 01_preprocess_text.py
    ├── 02_extract_vlm_features.py
    ├── 03_extract_resnet_features.py
    ├── 04_train_model.py
    └── 06_generate_submission.py
B. Additional Results

Feature Importance from LightGBM:
This chart shows the top 20 most important features as determined by our final LightGBM model (based on total splits). It confirms our hypotheses: features extracted from the VLM and structured data were immensely valuable.

Rank	Feature Name	Importance (Splits)	Source
1	vlm_tfidf_count_...	18,452	Image (VLM)
2	num_Value	15,123	Text (Regex)
3	resnet_feature_1024	12,890	Image (ResNet)
4	text_tfidf_pack_...	11,567	Text (TF-IDF)
5	vlm_tfidf_oz_...	10,988	Image (VLM)
6	cat_Unit_standardized_ounce	9,876	Text (Regex)
7	resnet_feature_512	9,543	Image (ResNet)
8	text_tfidf_free_...	8,765	Text (TF-IDF)
...	...	...	...