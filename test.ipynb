{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db0ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f54ccf",
   "metadata": {},
   "source": [
    "content scanning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a45882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.sparse import save_npz\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Helper functions (These are correct and do not need changes) ---\n",
    "\n",
    "def extract_structured_data(text):\n",
    "    if not isinstance(text, str): return pd.Series([np.nan, 'unknown'], index=['Value', 'Unit'])\n",
    "    value_match = re.search(r'Value:\\s*([\\d\\.]+)', text, re.IGNORECASE)\n",
    "    value = float(value_match.group(1)) if value_match else np.nan\n",
    "    unit_match = re.search(r'Unit:\\s*(\\w+)', text, re.IGNORECASE)\n",
    "    unit = unit_match.group(1) if unit_match else 'unknown'\n",
    "    return pd.Series([value, unit], index=['Value', 'Unit'])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'Value:.*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Unit:.*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def standardize_units(unit):\n",
    "    if not isinstance(unit, str): return 'unknown'\n",
    "    unit = unit.lower().strip()\n",
    "    unit_map = {'oz': 'ounce', 'ounces': 'ounce', 'fl oz': 'fl_oz', 'fz': 'fl_oz', 'ct': 'count', 'none': 'unknown'}\n",
    "    return unit_map.get(unit, unit)\n",
    "\n",
    "\n",
    "# --- CORRECTED: Main Preprocessing and Feature Engineering Pipeline ---\n",
    "\n",
    "def create_feature_pipeline(df, is_training=True):\n",
    "    \"\"\"\n",
    "    Creates and applies a full preprocessing pipeline.\n",
    "    If is_training=True, it fits the preprocessor and returns X, y, and the fitted preprocessor.\n",
    "    If is_training=False, it loads a pre-fitted preprocessor and transforms the data, returning only X.\n",
    "    \"\"\"\n",
    "    if df is None: return None\n",
    "\n",
    "    print(\"\\nExtracting 'Value' and 'Unit' from 'catalog_content'...\")\n",
    "    # Use reset_index to prevent potential alignment issues with concat\n",
    "    extracted_data = df['catalog_content'].apply(extract_structured_data)\n",
    "    df = pd.concat([df.reset_index(drop=True), extracted_data], axis=1)\n",
    "\n",
    "    if is_training:\n",
    "        # --- TRAINING MODE ---\n",
    "        # This block runs ONLY when you are processing the `train.csv` file.\n",
    "        df.dropna(subset=['price'], inplace=True)\n",
    "        y = np.log1p(df['price'])\n",
    "        sample_ids = df['sample_id']\n",
    "        # We drop the price here because it's the target, not a feature\n",
    "        features_df = df.drop(columns=['price', 'sample_id', 'image_link'])\n",
    "\n",
    "        print(\"\\nCleaning and preparing features for training...\")\n",
    "        features_df['cleaned_catalog'] = features_df['catalog_content'].apply(clean_text)\n",
    "        features_df['Unit_standardized'] = features_df['Unit'].apply(standardize_units)\n",
    "        \n",
    "        # Define the steps for the preprocessor\n",
    "        numeric_features = ['Value']\n",
    "        numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "        categorical_features = ['Unit_standardized']\n",
    "        categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "                                                  ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))])\n",
    "        text_features_col = 'cleaned_catalog'\n",
    "        text_transformer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "        \n",
    "        # Create the master preprocessor\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[('num', numeric_transformer, numeric_features),\n",
    "                          ('cat', categorical_transformer, categorical_features),\n",
    "                          ('text', text_transformer, text_features_col)],\n",
    "            remainder='drop', n_jobs=-1)\n",
    "        \n",
    "        print(\"Fitting preprocessor and transforming data...\")\n",
    "        # CRITICAL: Use `fit_transform` to learn from the training data\n",
    "        X = preprocessor.fit_transform(features_df)\n",
    "        \n",
    "        print(f\"--- Training Preprocessing Complete ---\")\n",
    "        print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "        \n",
    "        return X, y, sample_ids, preprocessor\n",
    "    \n",
    "    else:\n",
    "        # --- PREDICTION/TESTING MODE ---\n",
    "        # This block runs ONLY when you are processing the `test.csv` file.\n",
    "        sample_ids = df['sample_id']\n",
    "        # The 'price' column does not exist in test.csv, so no need to drop it\n",
    "        features_df = df.drop(columns=['sample_id', 'image_link'])\n",
    "\n",
    "        print(\"\\nCleaning and preparing features for testing...\")\n",
    "        features_df['cleaned_catalog'] = features_df['catalog_content'].apply(clean_text)\n",
    "        features_df['Unit_standardized'] = features_df['Unit'].apply(standardize_units)\n",
    "        \n",
    "        # Load the pre-fitted preprocessor that was saved during training\n",
    "        try:\n",
    "            preprocessor = joblib.load('processed_data/preprocessor.joblib')\n",
    "        except FileNotFoundError:\n",
    "            print(\"FATAL ERROR: preprocessor.joblib not found. You must run this script in 'train' mode first.\")\n",
    "            return None, None\n",
    "\n",
    "        print(\"Loading fitted preprocessor and transforming test data...\")\n",
    "        # CRITICAL: Use `transform` ONLY. Do not re-fit on test data.\n",
    "        X = preprocessor.transform(features_df)\n",
    "        \n",
    "        print(f\"--- Test Preprocessing Complete ---\")\n",
    "        print(f\"Shape of X: {X.shape}\")\n",
    "\n",
    "        return X, sample_ids\n",
    "\n",
    "# --- CORRECTED: Main execution block with a clear MODE switch ---\n",
    "if __name__ == '__main__':\n",
    "    # --- CHOOSE YOUR MODE ---\n",
    "    # Set to 'train' to process the full training data and create the artifacts.\n",
    "    # Set to 'test' to process the full testing data using the saved artifacts.\n",
    "    MODE = 'train' # <-- CHANGE THIS TO 'test' WHEN YOU ARE READY TO PROCESS THE TEST FILE\n",
    "\n",
    "    OUTPUT_DIR = 'processed_data'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    if MODE == 'train':\n",
    "        print(\"--- RUNNING IN TRAINING MODE ---\")\n",
    "        # Load the full training data which includes the price\n",
    "        # The `load_and_merge_data` function is no longer needed as `train.csv` is complete\n",
    "        try:\n",
    "            train_df = pd.read_csv(os.path.join('dataset', 'train.csv'))\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: `dataset/train.csv` not found. Please place the training file in the 'dataset' directory.\")\n",
    "            exit()\n",
    "            \n",
    "        X_processed, y_processed, ids_processed, preprocessor_fitted = create_feature_pipeline(train_df, is_training=True)\n",
    "        \n",
    "        if X_processed is not None:\n",
    "            # Save all the necessary artifacts for later\n",
    "            print(f\"\\nSaving processed training data and artifacts to '{OUTPUT_DIR}'...\")\n",
    "            save_npz(os.path.join(OUTPUT_DIR, 'X_processed_train.npz'), X_processed)\n",
    "            y_df = pd.DataFrame({'sample_id': ids_processed, 'price_log': y_processed})\n",
    "            y_df.to_csv(os.path.join(OUTPUT_DIR, 'y_processed_train.csv'), index=False)\n",
    "            joblib.dump(preprocessor_fitted, os.path.join(OUTPUT_DIR, 'preprocessor.joblib'))\n",
    "            print(\"✅ All training artifacts saved successfully.\")\n",
    "\n",
    "    elif MODE == 'test':\n",
    "        print(\"--- RUNNING IN TEST/PREDICTION MODE ---\")\n",
    "        # Load the test data which does NOT have a price\n",
    "        try:\n",
    "            test_df = pd.read_csv(os.path.join('dataset', 'test.csv'))\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: `dataset/test.csv` not found. Please place the testing file in the 'dataset' directory.\")\n",
    "            exit()\n",
    "        \n",
    "        X_processed, ids_processed = create_feature_pipeline(test_df, is_training=False)\n",
    "        \n",
    "        if X_processed is not None:\n",
    "            # Save the processed features for the test set\n",
    "            print(f\"\\nSaving processed test data to '{OUTPUT_DIR}'...\")\n",
    "            save_npz(os.path.join(OUTPUT_DIR, 'X_processed_test.npz'), X_processed)\n",
    "            ids_df = pd.DataFrame({'sample_id': ids_processed})\n",
    "            ids_df.to_csv(os.path.join(OUTPUT_DIR, 'ids_test.csv'), index=False)\n",
    "            print(\"✅ All test artifacts saved successfully.\")\n",
    "    else:\n",
    "        print(\"Invalid MODE selected. Please choose 'train' or 'test'.\")\n",
    "\n",
    "# The old save_processed_data function can be used as a helper if needed, but the logic is now in the main block.\n",
    "def save_processed_data(X, y, ids, preprocessor, output_dir='processed_data'):\n",
    "    # This function is now effectively replaced by the logic within the if/elif blocks\n",
    "    # but can be kept for reference or modularity if you refactor later.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4288ee7f",
   "metadata": {},
   "source": [
    "\"extract_text_from_images.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# --- 1. API Interaction Function (This remains the same) ---\n",
    "def get_local_vlm_description(image_url: str) -> str:\n",
    "    api_url = \"http://localhost:1234/v1/chat/completions\"\n",
    "    prompt_text = (\n",
    "        \"Analyze the image and complete the following fields based *only* on the visible text. \"\n",
    "        \"Add a very small targetted informative description about the image and the product inside.\\n\"\n",
    "        \"Brand: \\n\" \"Product: \\n\" \"Size/Quantity: \\n\" \"Features: \\n\"\n",
    "        \"above fileds are mandatory in that order itself\"\n",
    "    )\n",
    "    try:\n",
    "        with requests.Session() as session:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            response = session.get(image_url, timeout=30, headers=headers)\n",
    "            response.raise_for_status()\n",
    "        \n",
    "        base64_image = base64.b64encode(response.content).decode('utf-8')\n",
    "        payload = {\n",
    "            \"model\": \"smolvlm-500m-instruct\",\n",
    "            \"messages\": [\n",
    "                { \"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt_text}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}]}\n",
    "            ],\n",
    "            \"max_tokens\": 250, \"temperature\": 0.0\n",
    "        }\n",
    "        api_response = requests.post(api_url, headers={\"Content-Type\": \"application/json\"}, json=payload, timeout=60)\n",
    "        api_response.raise_for_status()\n",
    "        response_json = api_response.json()\n",
    "        if 'choices' in response_json and len(response_json['choices']) > 0:\n",
    "            content = response_json['choices'][0]['message']['content']\n",
    "            return \" \".join(content.strip().splitlines())\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "# --- Main execution block with LIGHTWEIGHT APPEND-ONLY REPAIR ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Starting TARGETED REPAIR for missing VLM descriptions (Lightweight Mode) ---\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    BATCH_SIZE = 20\n",
    "    TRAIN_CSV_PATH = os.path.join('dataset', 'train.csv')\n",
    "    TEST_CSV_PATH = os.path.join('dataset', 'test.csv')\n",
    "    OUTPUT_DIR = 'processed_data'\n",
    "    VLM_CSV_PATH = os.path.join(OUTPUT_DIR, 'smolvlm_extracted_features.csv') # Use a consistent name\n",
    "\n",
    "    # --- Step 1: Find missing descriptions ---\n",
    "    print(f\"Loading existing VLM file to find gaps: {VLM_CSV_PATH}\")\n",
    "    try:\n",
    "        df_vlm = pd.read_csv(VLM_CSV_PATH, engine='python')\n",
    "        missing_mask = df_vlm['smolvlm_description'].isna() | (df_vlm['smolvlm_description'].str.strip() == '')\n",
    "        missing_ids = set(df_vlm[missing_mask]['sample_id'])\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        print(\"VLM file not found or is empty. Please run the initial generation script first.\")\n",
    "        exit()\n",
    "\n",
    "    if not missing_ids:\n",
    "        print(\"✅ No missing descriptions found. The file is complete!\")\n",
    "        exit()\n",
    "    print(f\"Found {len(missing_ids)} products with missing descriptions to repair.\")\n",
    "\n",
    "    # --- Step 2: Create master lookup for image links ---\n",
    "    print(\"Creating a master lookup for image links...\")\n",
    "    df_train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "    df_test = pd.read_csv(TEST_CSV_PATH)\n",
    "    df_all = pd.concat([\n",
    "        df_train[['sample_id', 'image_link']],\n",
    "        df_test[['sample_id', 'image_link']]\n",
    "    ]).drop_duplicates(subset=['sample_id']).set_index('sample_id')\n",
    "    \n",
    "    df_todo = df_all[df_all.index.isin(missing_ids)]\n",
    "\n",
    "    if df_todo.empty:\n",
    "        print(\"Could not find image links for any of the missing IDs. Exiting.\")\n",
    "        exit()\n",
    "    print(f\"Found {len(df_todo)} matching image links to re-process.\")\n",
    "\n",
    "    # --- Step 3: Process missing items and APPEND to the same CSV ---\n",
    "    image_urls = df_todo['image_link'].tolist()\n",
    "    sample_ids = df_todo.index.tolist()\n",
    "    \n",
    "    # Open the file in append mode. This is lightweight.\n",
    "    with open(VLM_CSV_PATH, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=BATCH_SIZE) as executor:\n",
    "            future_to_id = {executor.submit(get_local_vlm_description, url): sid for url, sid in zip(image_urls, sample_ids)}\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_id), total=len(image_urls), desc=\"Repairing Descriptions\"):\n",
    "                sample_id = future_to_id[future]\n",
    "                try:\n",
    "                    description = future.result()\n",
    "                    # Append the new (or re-processed) result. This will create duplicates.\n",
    "                    writer.writerow([sample_id, description])\n",
    "                except Exception as exc:\n",
    "                    print(f'\\nGenerated an exception for item {sample_id}: {exc}')\n",
    "                    writer.writerow([sample_id, \"\"]) # Append a blank row on error\n",
    "\n",
    "    print(\"\\n✅ Repair process complete. Appended new results. Now de-duplicating the file...\")\n",
    "\n",
    "    # --- Step 4: Final Cleanup Step ---\n",
    "    # This reads the now-larger file once, de-duplicates it, and saves the clean version.\n",
    "    # This is much more efficient than doing it in a loop.\n",
    "    df_final = pd.read_csv(VLM_CSV_PATH, engine='python')\n",
    "    # Keep the LAST entry for each sample_id, as it's the most recent (and hopefully correct) one\n",
    "    df_cleaned = df_final.drop_duplicates(subset=['sample_id'], keep='last')\n",
    "    \n",
    "    # Overwrite the file with the final, clean version\n",
    "    df_cleaned.to_csv(VLM_CSV_PATH, index=False)\n",
    "    \n",
    "    final_missing = df_cleaned['smolvlm_description'].isna().sum() + (df_cleaned['smolvlm_description'].str.strip() == '').sum()\n",
    "    print(f\"✅ Cleanup complete. The file '{VLM_CSV_PATH}' is now updated and de-duplicated.\")\n",
    "    print(f\"   → Total missing descriptions remaining: {final_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9245b716",
   "metadata": {},
   "source": [
    "handling missing image data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13cfa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# --- 1. API Interaction Function (This remains the same) ---\n",
    "def get_local_vlm_description(image_url: str) -> str:\n",
    "    api_url = \"http://localhost:1234/v1/chat/completions\"\n",
    "    prompt_text = (\n",
    "        \"Analyze the image and complete the following fields based *only* on the visible text. \"\n",
    "        \"Add a very small targetted informative description about the image and the product inside.\\n\"\n",
    "        \"Brand: \\n\" \"Product: \\n\" \"Size/Quantity: \\n\" \"Features: \\n\"\n",
    "        \"above fileds are mandatory in that order itself\"\n",
    "    )\n",
    "    try:\n",
    "        with requests.Session() as session:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            response = session.get(image_url, timeout=30, headers=headers)\n",
    "            response.raise_for_status()\n",
    "        \n",
    "        base64_image = base64.b64encode(response.content).decode('utf-8')\n",
    "        payload = {\n",
    "            \"model\": \"smolvlm-256m-instruct\",\n",
    "            \"messages\": [\n",
    "                { \"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt_text}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}]}\n",
    "            ],\n",
    "            \"max_tokens\": 250, \"temperature\": 0.0\n",
    "        }\n",
    "        api_response = requests.post(api_url, headers={\"Content-Type\": \"application/json\"}, json=payload, timeout=60)\n",
    "        api_response.raise_for_status()\n",
    "        response_json = api_response.json()\n",
    "        if 'choices' in response_json and len(response_json['choices']) > 0:\n",
    "            content = response_json['choices'][0]['message']['content']\n",
    "            return \" \".join(content.strip().splitlines())\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "# --- Main execution block with LIGHTWEIGHT APPEND-ONLY REPAIR ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Starting TARGETED REPAIR for missing VLM descriptions (Lightweight Mode) ---\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    BATCH_SIZE = 20\n",
    "    TRAIN_CSV_PATH = os.path.join('dataset', 'train.csv')\n",
    "    TEST_CSV_PATH = os.path.join('dataset', 'test.csv')\n",
    "    OUTPUT_DIR = 'processed_data'\n",
    "    VLM_CSV_PATH = os.path.join(OUTPUT_DIR, 'smolvlm_extracted_features[1].csv') # Use a consistent name\n",
    "\n",
    "    # --- Step 1: Find missing descriptions ---\n",
    "    print(f\"Loading existing VLM file to find gaps: {VLM_CSV_PATH}\")\n",
    "    try:\n",
    "        df_vlm = pd.read_csv(VLM_CSV_PATH, engine='python')\n",
    "        missing_mask = df_vlm['smolvlm_description'].isna() | (df_vlm['smolvlm_description'].str.strip() == '')\n",
    "        missing_ids = set(df_vlm[missing_mask]['sample_id'])\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        print(\"VLM file not found or is empty. Please run the initial generation script first.\")\n",
    "        exit()\n",
    "\n",
    "    if not missing_ids:\n",
    "        print(\"✅ No missing descriptions found. The file is complete!\")\n",
    "        exit()\n",
    "    print(f\"Found {len(missing_ids)} products with missing descriptions to repair.\")\n",
    "\n",
    "    # --- Step 2: Create master lookup for image links ---\n",
    "    print(\"Creating a master lookup for image links...\")\n",
    "    df_train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "    df_test = pd.read_csv(TEST_CSV_PATH)\n",
    "    df_all = pd.concat([\n",
    "        df_train[['sample_id', 'image_link']],\n",
    "        df_test[['sample_id', 'image_link']]\n",
    "    ]).drop_duplicates(subset=['sample_id']).set_index('sample_id')\n",
    "    \n",
    "    df_todo = df_all[df_all.index.isin(missing_ids)]\n",
    "\n",
    "    if df_todo.empty:\n",
    "        print(\"Could not find image links for any of the missing IDs. Exiting.\")\n",
    "        exit()\n",
    "    print(f\"Found {len(df_todo)} matching image links to re-process.\")\n",
    "\n",
    "    # --- Step 3: Process missing items and APPEND to the same CSV ---\n",
    "    image_urls = df_todo['image_link'].tolist()\n",
    "    sample_ids = df_todo.index.tolist()\n",
    "    \n",
    "    # Open the file in append mode. This is lightweight.\n",
    "    with open(VLM_CSV_PATH, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=BATCH_SIZE) as executor:\n",
    "            future_to_id = {executor.submit(get_local_vlm_description, url): sid for url, sid in zip(image_urls, sample_ids)}\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_id), total=len(image_urls), desc=\"Repairing Descriptions\"):\n",
    "                sample_id = future_to_id[future]\n",
    "                try:\n",
    "                    description = future.result()\n",
    "                    # Append the new (or re-processed) result. This will create duplicates.\n",
    "                    writer.writerow([sample_id, description])\n",
    "                except Exception as exc:\n",
    "                    print(f'\\nGenerated an exception for item {sample_id}: {exc}')\n",
    "                    writer.writerow([sample_id, \"\"]) # Append a blank row on error\n",
    "\n",
    "    print(\"\\n✅ Repair process complete. Appended new results. Now de-duplicating the file...\")\n",
    "\n",
    "    # --- Step 4: Final Cleanup Step ---\n",
    "    # This reads the now-larger file once, de-duplicates it, and saves the clean version.\n",
    "    # This is much more efficient than doing it in a loop.\n",
    "    df_final = pd.read_csv(VLM_CSV_PATH, engine='python')\n",
    "    # Keep the LAST entry for each sample_id, as it's the most recent (and hopefully correct) one\n",
    "    df_cleaned = df_final.drop_duplicates(subset=['sample_id'], keep='last')\n",
    "    \n",
    "    # Overwrite the file with the final, clean version\n",
    "    df_cleaned.to_csv(VLM_CSV_PATH, index=False)\n",
    "    \n",
    "    final_missing = df_cleaned['smolvlm_description'].isna().sum() + (df_cleaned['smolvlm_description'].str.strip() == '').sum()\n",
    "    print(f\"✅ Cleanup complete. The file '{VLM_CSV_PATH}' is now updated and de-duplicated.\")\n",
    "    print(f\"   → Total missing descriptions remaining: {final_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d203ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_and_deduplicate_csv(file_path):\n",
    "    \"\"\"\n",
    "    Cleans a VLM/OCR feature CSV by:\n",
    "    1. Removing ALL rows that have a missing or empty description.\n",
    "    2. De-duplicating any remaining sample_ids, keeping the last entry.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Aggressive Cleanup for: {file_path} ---\")\n",
    "\n",
    "    # --- Step 1: Load the CSV file ---\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, engine='python')\n",
    "        initial_rows = len(df)\n",
    "        print(f\"Successfully loaded {initial_rows} rows.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found. Nothing to clean.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Step 2: Remove ALL rows with missing descriptions ---\n",
    "    # Create a mask to identify rows where 'smolvlm_description' is NaN (empty) or just whitespace\n",
    "    empty_mask = df['smolvlm_description'].isna() | (df['smolvlm_description'].str.strip() == '')\n",
    "    \n",
    "    num_empty = empty_mask.sum()\n",
    "    \n",
    "    if num_empty > 0:\n",
    "        print(f\"Found {num_empty} rows with empty or missing descriptions.\")\n",
    "        # The `~` operator inverts the mask, keeping all rows that are NOT empty.\n",
    "        df_cleaned = df[~empty_mask].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "        print(f\"   → Removed {num_empty} empty rows. Remaining rows: {len(df_cleaned)}\")\n",
    "    else:\n",
    "        print(\"✅ No empty rows found to remove.\")\n",
    "        df_cleaned = df.copy()\n",
    "\n",
    "    # --- Step 3: De-duplicate the remaining data ---\n",
    "    # After removing the empty rows, we might still have duplicates (e.g., if an ID was processed twice successfully).\n",
    "    # We will keep the LAST entry for each sample_id, as it is the most recent.\n",
    "    \n",
    "    initial_dedupe_rows = len(df_cleaned)\n",
    "    if df_cleaned.duplicated(subset=['sample_id']).any():\n",
    "        print(f\"Found duplicate sample_ids in the remaining data. Performing final de-duplication...\")\n",
    "        df_final = df_cleaned.drop_duplicates(subset=['sample_id'], keep='last')\n",
    "        num_duplicates_removed = initial_dedupe_rows - len(df_final)\n",
    "        print(f\"   → Removed {num_duplicates_removed} older duplicate entries.\")\n",
    "    else:\n",
    "        print(\"✅ No remaining duplicates found.\")\n",
    "        df_final = df_cleaned\n",
    "\n",
    "    # --- Step 4: Save the cleaned DataFrame back to the same file ---\n",
    "    # This overwrites the old, messy file with the new, clean one.\n",
    "    df_final.to_csv(file_path, index=False)\n",
    "    \n",
    "    final_rows = len(df_final)\n",
    "    print(\"\\n--- ✅ Cleanup Complete ---\")\n",
    "    print(f\"   → Initial rows: {initial_rows}\")\n",
    "    print(f\"   → Final rows:   {final_rows}\")\n",
    "    print(f\"   → Total rows removed: {initial_rows - final_rows}\")\n",
    "    print(f\"Successfully saved the cleaned data back to '{file_path}'.\")\n",
    "\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == '__main__':\n",
    "    # Define the path to your CSV file\n",
    "    # I've updated the filename to match the one you used in your previous code.\n",
    "    VLM_CSV_PATH = os.path.join('processed_data', 'smolvlm_extracted_features[1].csv') \n",
    "    \n",
    "    clean_and_deduplicate_csv(VLM_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82104dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# --- 1. API Interaction Function for Ollama ---\n",
    "\n",
    "def get_ollama_summary(catalog_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends catalog text to a local Ollama model and gets a single-line summary.\n",
    "    \"\"\"\n",
    "    if not isinstance(catalog_text, str) or not catalog_text.strip():\n",
    "        return \"\" # Return empty if the input is empty\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following product text, generate a single, concise, one-sentence description.\n",
    "    Mention the brand, the main product type, and one or two key features if possible.\n",
    "    Do not use bullet points. Do not repeat the input.\n",
    "\n",
    "    Product Text:\n",
    "    ---\n",
    "    {catalog_text[:2000]} \n",
    "    ---\n",
    "\n",
    "    One-sentence description:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='tinyllama:latest',\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            options={'temperature': 0.2}\n",
    "        )\n",
    "        summary = response['message']['content']\n",
    "        cleaned_summary = summary.strip().replace('\\n', ' ').replace('\"', '')\n",
    "        return cleaned_summary\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "# --- Main execution block with TARGETED REPAIR logic ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Starting SMART REPAIR for smolvlm.csv using Ollama ---\")\n",
    "    print(\"This will fill gaps using summarized catalog_content.\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    CONCURRENT_REQUESTS = 8 # Number of parallel requests to Ollama\n",
    "    TRAIN_CSV_PATH = os.path.join('dataset', 'train.csv')\n",
    "    TEST_CSV_PATH = os.path.join('dataset', 'test.csv')\n",
    "    OUTPUT_DIR = 'processed_data'\n",
    "    VLM_CSV_PATH = os.path.join(OUTPUT_DIR, 'smolvlm_extracted_features.csv') \n",
    "\n",
    "    # --- Step 1: Find missing descriptions in the existing VLM file ---\n",
    "    print(f\"Loading existing VLM file to find gaps: {VLM_CSV_PATH}\")\n",
    "    try:\n",
    "        df_vlm = pd.read_csv(VLM_CSV_PATH, engine='python')\n",
    "        # Identify rows where description is missing or blank\n",
    "        missing_mask = df_vlm['smolvlm_description'].isna() | (df_vlm['smolvlm_description'].str.strip() == '')\n",
    "        missing_ids = set(df_vlm[missing_mask]['sample_id'])\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        print(f\"VLM file not found or is empty. Cannot repair. Please run the initial VLM script first.\")\n",
    "        exit()\n",
    "\n",
    "    if not missing_ids:\n",
    "        print(\"✅ No missing descriptions found. The file is already complete!\")\n",
    "        exit()\n",
    "    print(f\"Found {len(missing_ids)} products with missing descriptions to repair.\")\n",
    "\n",
    "    # --- Step 2: Create master lookup to get the `catalog_content` for the missing IDs ---\n",
    "    print(\"Creating a master lookup for catalog content...\")\n",
    "    df_train = pd.read_csv(TRAIN_CSV_PATH, usecols=['sample_id', 'catalog_content'])\n",
    "    df_test = pd.read_csv(TEST_CSV_PATH, usecols=['sample_id', 'catalog_content'])\n",
    "    df_all = pd.concat([df_train, df_test]).drop_duplicates(subset=['sample_id']).set_index('sample_id')\n",
    "    \n",
    "    # Filter the master list to get ONLY the items we need to process\n",
    "    df_todo = df_all[df_all.index.isin(missing_ids)]\n",
    "\n",
    "    if df_todo.empty:\n",
    "        print(\"Could not find catalog_content for any of the missing IDs. Exiting.\")\n",
    "        exit()\n",
    "    print(f\"Found {len(df_todo)} matching catalog_content entries to summarize.\")\n",
    "\n",
    "    # --- Step 3: Process only the missing items' content and APPEND to the CSV ---\n",
    "    catalog_texts = df_todo['catalog_content'].tolist()\n",
    "    sample_ids = df_todo.index.tolist()\n",
    "    \n",
    "    # Open the file in append mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82061478",
   "metadata": {},
   "source": [
    "backup for solvlm2-500m-video-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579cf6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def flatten_catalog_content(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans and consolidates the raw catalog_content into a single line of text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def find_and_replace_raw_catalog_content(\n",
    "    train_csv_path=os.path.join('dataset', 'train.csv'),\n",
    "    test_csv_path=os.path.join('dataset', 'test.csv'),\n",
    "    vlm_csv_path=os.path.join('processed_data', 'smolvlm_extracted_features.csv')\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds rows in the VLM file that were filled with raw catalog_content\n",
    "    and replaces them with a cleaned, single-line version.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting 'Find and Replace' for: {vlm_csv_path} ---\")\n",
    "\n",
    "    # --- Step 1: Load the existing VLM file that needs fixing ---\n",
    "    print(f\"1. Loading existing VLM file from: {vlm_csv_path}\")\n",
    "    try:\n",
    "        df_vlm = pd.read_csv(vlm_csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{vlm_csv_path}' was not found. Nothing to fix.\")\n",
    "        return\n",
    "    print(f\"   ✓ Successfully loaded {len(df_vlm)} rows.\")\n",
    "\n",
    "    # --- Step 2: Identify the \"bad\" rows ---\n",
    "    # A row is considered \"bad\" if its description contains characters that suggest it's raw,\n",
    "    # uncleaned catalog_content, like newlines or HTML tags.\n",
    "    # We also check for very long descriptions as another signal.\n",
    "    print(\"2. Identifying rows that need to be cleaned...\")\n",
    "    bad_row_mask = (\n",
    "        df_vlm['smolvlm_description'].str.contains('\\n|<br>', regex=True, na=False) |\n",
    "        (df_vlm['smolvlm_description'].str.len() > 1000) # Descriptions longer than 1000 chars are likely raw content\n",
    "    )\n",
    "    \n",
    "    num_bad_rows = bad_row_mask.sum()\n",
    "\n",
    "    if num_bad_rows == 0:\n",
    "        print(\"✅ No rows with raw catalog_content detected. The file appears to be clean.\")\n",
    "        return\n",
    "\n",
    "    print(f\"   → Found {num_bad_rows} rows that need to be replaced with a clean, single-line summary.\")\n",
    "    \n",
    "    # Get the list of IDs we need to fix\n",
    "    ids_to_fix = df_vlm[bad_row_mask]['sample_id'].tolist()\n",
    "\n",
    "    # --- Step 3: Load the master scaffold to get the original catalog_content ---\n",
    "    print(\"3. Loading master data to get the source catalog_content...\")\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_csv_path, usecols=['sample_id', 'catalog_content'])\n",
    "        df_test = pd.read_csv(test_csv_path, usecols=['sample_id', 'catalog_content'])\n",
    "        df_all_products = pd.concat([df_train, df_test]).drop_duplicates(subset=['sample_id'])\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"FATAL ERROR: Could not find a master CSV file. {e}\")\n",
    "        return\n",
    "\n",
    "    # Filter the master list to get ONLY the content for the rows we need to fix\n",
    "    df_source_content = df_all_products[df_all_products['sample_id'].isin(ids_to_fix)]\n",
    "    \n",
    "    if df_source_content.empty:\n",
    "        print(\"Warning: Could not find source content for the identified bad rows. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    # --- Step 4: Generate the clean, single-line replacements ---\n",
    "    print(f\"4. Generating {len(df_source_content)} single-line replacements...\")\n",
    "    # Create a dictionary mapping sample_id to its new, clean description\n",
    "    replacements = {\n",
    "        row['sample_id']: flatten_catalog_content(row['catalog_content'])\n",
    "        for _, row in df_source_content.iterrows()\n",
    "    }\n",
    "    print(\"   ✓ Replacements generated.\")\n",
    "\n",
    "    # --- Step 5: Update the original DataFrame and save ---\n",
    "    print(\"5. Replacing bad rows and saving the final file...\")\n",
    "    # Set 'sample_id' as the index for efficient updating\n",
    "    df_vlm.set_index('sample_id', inplace=True)\n",
    "    \n",
    "    # Create a pandas Series from the replacements dictionary\n",
    "    replacement_series = pd.Series(replacements, name='smolvlm_description')\n",
    "    \n",
    "    # The .update() method will overwrite the values in df_vlm for the matching indices\n",
    "    df_vlm.update(replacement_series)\n",
    "    \n",
    "    # Bring 'sample_id' back as a column\n",
    "    df_vlm.reset_index(inplace=True)\n",
    "\n",
    "    # Overwrite the old file with the now-repaired version\n",
    "    df_vlm.to_csv(vlm_csv_path, index=False)\n",
    "\n",
    "    print(\"\\n--- ✅ Process Complete ---\")\n",
    "    print(f\"The file '{vlm_csv_path}' has been successfully repaired.\")\n",
    "    print(f\"   → Replaced {len(replacements)} messy descriptions with clean, single-line versions.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Make sure this path is correct\n",
    "    repair_and_fill_vlm_csv(vlm_csv_path=os.path.join('processed_data', 'smolvlm_extracted_features[1].csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6aaea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# --- 1. API Interaction Function (with the new, simplified prompt) ---\n",
    "\n",
    "def get_local_vlm_description(image_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads an image, encodes it, and gets a structured description from a local\n",
    "    OpenAI-compatible VLM server (like LM Studio).\n",
    "    \"\"\"\n",
    "    api_url = \"http://localhost:1234/v1/chat/completions\"\n",
    "    \n",
    "    # --- THIS IS THE NEW, SIMPLIFIED PROMPT ---\n",
    "    # It is designed for smaller models that struggle with complex instructions.\n",
    "    # This \"fill-in-the-blanks\" format is much easier for them to follow.\n",
    "    prompt_text = (\n",
    "        \"Analyze the image and extract the exact text visible in the image. \"\n",
    "        \"Include all visible text.\\n\\n\"\n",
    "        \"Brand: \\n\"\n",
    "        \"Product: \\n\"\n",
    "        \"Size/Quantity: \\n\"\n",
    "        \"Features: \\n\"\n",
    "        \"a short description of the product\"\n",
    "        \"also give a short description of the features from the image and the product\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(image_url, timeout=20, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        base64_image = base64.b64encode(response.content).decode('utf-8')\n",
    "\n",
    "        payload = {\n",
    "            \"model\": \"smolvlm-256m-instruct\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt_text},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": { \"url\": f\"data:image/jpeg;base64,{base64_image}\" }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": 250,\n",
    "            \"temperature\": 0.0 # Keep temperature at 0 for factual extraction\n",
    "        }\n",
    "\n",
    "        api_response = requests.post(api_url, headers={\"Content-Type\": \"application/json\"}, json=payload)\n",
    "        api_response.raise_for_status()\n",
    "        response_json = api_response.json()\n",
    "\n",
    "        if 'choices' in response_json and len(response_json['choices']) > 0:\n",
    "            content = response_json['choices'][0]['message']['content']\n",
    "            # We combine the lines into a single string for easy processing later\n",
    "            return \" \".join(content.strip().splitlines())\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "# --- Main execution block with Parallel Batch Processing and CHECKPOINTING ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Starting PARALLEL BATCH Feature Extraction with CHECKPOINTING ---\")\n",
    "    print(\"IMPORTANT: Ensure your LM Studio server is running.\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    BATCH_SIZE = 20\n",
    "    TRAIN_CSV_PATH = os.path.join('dataset', 'train.csv')\n",
    "    TEST_CSV_PATH = os.path.join('dataset', 'test.csv')\n",
    "    OUTPUT_DIR = 'processed_data'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # --- NEW: Define the checkpoint file path ---\n",
    "    CHECKPOINT_FILE = os.path.join(OUTPUT_DIR, 'smolvlm_extracted_features[1].csv')\n",
    "\n",
    "    print(f\"Loading CSV files: {TRAIN_CSV_PATH} and {TEST_CSV_PATH}...\")\n",
    "    try:\n",
    "        df_train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "        df_test = pd.read_csv(TEST_CSV_PATH)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Make sure your train.csv and test.csv are in the 'dataset' folder.\")\n",
    "        exit()\n",
    "\n",
    "    df_all = pd.concat([\n",
    "        df_train[['sample_id', 'image_link']],\n",
    "        df_test[['sample_id', 'image_link']]\n",
    "    ]).drop_duplicates(subset=['sample_id']).reset_index(drop=True)\n",
    "\n",
    "    processed_ids = set()\n",
    "    # --- NEW: Check if a checkpoint file exists and load it ---\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        print(f\"Resuming from existing checkpoint file: {CHECKPOINT_FILE}\")\n",
    "        df_checkpoint = pd.read_csv(CHECKPOINT_FILE)\n",
    "        processed_ids = set(df_checkpoint['sample_id'])\n",
    "        print(f\"Found {len(processed_ids)} products already processed.\")\n",
    "    else:\n",
    "        print(\"No checkpoint file found. Starting a new run.\")\n",
    "        # Create an empty file with headers if it doesn't exist\n",
    "        pd.DataFrame(columns=['sample_id', 'smolvlm_description']).to_csv(CHECKPOINT_FILE, index=False)\n",
    "\n",
    "    # --- NEW: Filter out the products that have already been processed ---\n",
    "    df_todo = df_all[~df_all['sample_id'].isin(processed_ids)]\n",
    "\n",
    "    if df_todo.empty:\n",
    "        print(\"All products have already been processed. Nothing to do.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Found a total of {len(df_todo)} new products to process.\")\n",
    "\n",
    "    # --- Process the remaining items in parallel batches ---\n",
    "    image_urls = df_todo['image_link'].tolist()\n",
    "    sample_ids = df_todo['sample_id'].tolist()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=BATCH_SIZE) as executor:\n",
    "        # Map futures to sample_ids to keep track of them\n",
    "        future_to_id = {executor.submit(get_local_vlm_description, url): sid for url, sid in zip(image_urls, sample_ids)}\n",
    "        \n",
    "        results_to_append = []\n",
    "\n",
    "        for future in tqdm(as_completed(future_to_id), total=len(image_urls), desc=\"Processing Batches\"):\n",
    "            sample_id = future_to_id[future]\n",
    "            try:\n",
    "                description = future.result()\n",
    "                results_to_append.append({'sample_id': sample_id, 'smolvlm_description': description})\n",
    "\n",
    "                # --- NEW: Save to CSV in batches to create checkpoints ---\n",
    "                if len(results_to_append) >= BATCH_SIZE:\n",
    "                    # Append the batch of results to the CSV file\n",
    "                    pd.DataFrame(results_to_append).to_csv(CHECKPOINT_FILE, mode='a', header=False, index=False)\n",
    "                    results_to_append = [] # Clear the batch\n",
    "                    \n",
    "            except Exception as exc:\n",
    "                print(f'\\nGenerated an exception for item {sample_id}: {exc}')\n",
    "                results_to_append.append({'sample_id': sample_id, 'smolvlm_description': \"\"})\n",
    "\n",
    "    # --- NEW: Save any remaining results after the loop finishes ---\n",
    "    if results_to_append:\n",
    "        pd.DataFrame(results_to_append).to_csv(CHECKPOINT_FILE, mode='a', header=False, index=False)\n",
    "\n",
    "    print(f\"\\n✅ Parallel Batch VLM feature extraction complete. All descriptions saved to '{CHECKPOINT_FILE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc93e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz, hstack\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "# --- 1. Re-usable Components ---\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    ratio = np.where(denominator == 0, 0, numerator / denominator)\n",
    "    return np.mean(ratio) * 100\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    \"\"\"A consistent cleaning function for both text sources.\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'<.*?>|Value:.*|Unit:.*', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# --- 2. Load and Combine All Data Sources ---\n",
    "\n",
    "def load_and_combine_all_features(data_dir='processed_data'):\n",
    "    \"\"\"\n",
    "    Loads all preprocessed features and combines them.\n",
    "    Returns the combined feature matrix and the original DataFrame with prices.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"--- Loading All Preprocessed Data ---\")\n",
    "        \n",
    "        X_text_struct = load_npz(f'{data_dir}/X_processed.npz')\n",
    "        y_df = pd.read_csv(f'{data_dir}/y_processed.csv').set_index('sample_id')\n",
    "        \n",
    "        vlm_df = pd.read_csv(f'{data_dir}/smolvlm_extracted_features.csv').set_index('sample_id')\n",
    "        vlm_df = vlm_df.reindex(y_df.index).fillna('')\n",
    "        vlm_df['cleaned_vlm'] = vlm_df['smolvlm_description'].apply(clean_text)\n",
    "        \n",
    "        vlm_vectorizer = TfidfVectorizer(max_features=1500, ngram_range=(1, 2))\n",
    "        X_vlm = vlm_vectorizer.fit_transform(vlm_df['cleaned_vlm'])\n",
    "        \n",
    "        X_images = np.load(f'{data_dir}/X_image_features.npy')\n",
    "        all_ids_df = pd.read_csv(f'{data_dir}/all_sample_ids.csv').set_index('sample_id')\n",
    "        image_features_df = pd.DataFrame(X_images, index=all_ids_df.index)\n",
    "        X_images_aligned = image_features_df.reindex(y_df.index).values\n",
    "        \n",
    "        X_final_combined = hstack([X_text_struct, X_vlm, X_images_aligned]).tocsr()\n",
    "\n",
    "        print(f\"✅ Data loaded and combined successfully! Final shape: {X_final_combined.shape}\")\n",
    "        \n",
    "        # We need the original dataframe with IDs and prices for splitting and scoring\n",
    "        full_df = pd.read_csv(f'{data_dir}/y_processed.csv')\n",
    "        print(\"Vectorizing VLM text...\")\n",
    "        vlm_vectorizer = TfidfVectorizer(max_features=1500, ngram_range=(1, 2))\n",
    "        # This line was changed: we fit_transform on the training data\n",
    "        X_vlm = vlm_vectorizer.fit_transform(vlm_df['cleaned_vlm'])\n",
    "        \n",
    "        # --- ADD THIS LINE ---\n",
    "        joblib.dump(vlm_vectorizer, f'{data_dir}/vlm_vectorizer.joblib')\n",
    "        print(\"Saved VLM TF-IDF vectorizer.\")\n",
    "        return X_final_combined, full_df\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Please ensure you have run all three preprocessing scripts first.\")\n",
    "        return None, None\n",
    "\n",
    "# --- 3. Full Train, Predict, and Evaluate Pipeline ---\n",
    "\n",
    "def run_full_pipeline(X, df):\n",
    "    \"\"\"\n",
    "    Trains on a subset, predicts on a holdout, saves a submission file, and scores it.\n",
    "    \"\"\"\n",
    "    if X is None or df is None: return\n",
    "\n",
    "    print(\"\\n--- Simulating Competition Workflow ---\")\n",
    "    \n",
    "    # --- Step 1: Split the data into a training set and a \"pretend\" test set ---\n",
    "    # We split based on the dataframe to keep track of IDs\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Get the corresponding rows from the sparse matrix X\n",
    "    X_train = X[train_df.index]\n",
    "    y_train = train_df['price_log']\n",
    "    \n",
    "    X_test = X[test_df.index]\n",
    "    # We will use this later to score our predictions\n",
    "    y_test_ground_truth_log = test_df['price_log'] \n",
    "\n",
    "    print(f\"Training on {len(train_df)} samples, predicting on {len(test_df)} samples.\")\n",
    "\n",
    "    # --- Step 2: Train the model ONLY on the training set ---\n",
    "    print(\"\\nTraining a LightGBM Regressor model...\")\n",
    "    lgbm = lgb.LGBMRegressor(\n",
    "        objective='regression_l1', metric='mae', n_estimators=1000,\n",
    "        learning_rate=0.05, num_leaves=31, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    lgbm.fit(X_train, y_train, eval_set=[(X_test, y_test_ground_truth_log)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # --- Step 3: Make predictions on the \"pretend\" test set ---\n",
    "    print(\"Making predictions on the holdout test set...\")\n",
    "    predictions_log = lgbm.predict(X_test)\n",
    "    \n",
    "    # Inverse transform to get actual prices\n",
    "    predictions_actual = np.expm1(predictions_log)\n",
    "    predictions_actual[predictions_actual < 0] = 0 # Enforce non-negative constraint\n",
    "\n",
    "    # --- Step 4: Create and save the submission CSV file ---\n",
    "    submission_df = pd.DataFrame({\n",
    "        'sample_id': test_df['sample_id'],\n",
    "        'price': predictions_actual\n",
    "    })\n",
    "    \n",
    "    submission_path = 'sample_submission.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"✅ Submission file created at: '{submission_path}'\")\n",
    "\n",
    "    # --- Step 5: Score the submission file against the ground truth ---\n",
    "    print(\"\\n--- Scoring Submission File ---\")\n",
    "    \n",
    "    # Load the ground truth from the original file\n",
    "    ground_truth_df = pd.read_csv('68e8d1d70b66d_student_resource/student_resource/dataset/sample_test_out.csv')\n",
    "    \n",
    "    # Merge our predictions with the ground truth\n",
    "    # This mimics exactly how the competition leaderboard is calculated\n",
    "    merged_score_df = pd.merge(submission_df, ground_truth_df, on='sample_id', suffixes=('_pred', '_true'))\n",
    "    \n",
    "    if len(merged_score_df) != len(test_df):\n",
    "        print(\"Warning: The number of predictions does not match the test set size!\")\n",
    "\n",
    "    # Calculate the final SMAPE score\n",
    "    final_smape_score = smape(merged_score_df['price_true'], merged_score_df['price_pred'])\n",
    "    \n",
    "    print(\"\\n--- FINAL SIMULATED SCORE ---\")\n",
    "    print(f\"SMAPE against sample_test_out.csv: {final_smape_score:.4f}%\")\n",
    "    \n",
    "    return lgbm\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == '__main__':\n",
    "    # Load all features and the corresponding dataframe with IDs and prices\n",
    "    X_final, df_final = load_and_combine_all_features()\n",
    "    \n",
    "    # Run the full simulation\n",
    "    final_model = run_full_pipeline(X_final, df_final)\n",
    "    \n",
    "    if final_model:\n",
    "        joblib.dump(final_model, 'final_lgbm_model_for_submission.joblib')\n",
    "        print(\"\\n✅ Final trained model saved as 'final_lgbm_model_for_submission.joblib'\")\n",
    "        print(\"You can use this model to predict on the real 'test.csv' for the competition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc77c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import os # Added for path joining\n",
    "from sklearn.metrics import r2_score # Added for R-squared score\n",
    "\n",
    "# --- 1. Load All Necessary Artifacts and Helper Functions ---\n",
    "print(\"--- Loading Artifacts for Prediction ---\")\n",
    "\n",
    "# --- Helper Functions (must be identical to training) ---\n",
    "# NOTE: The SMAPE function is now defined here for scoring\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    ratio = np.where(denominator == 0, 0, numerator / denominator)\n",
    "    return np.mean(ratio) * 100\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'<.*?>|Value:.*|Unit:.*', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words and len(w) > 2]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def extract_structured_data(text):\n",
    "    if not isinstance(text, str): return pd.Series([np.nan, 'unknown'], index=['Value', 'Unit'])\n",
    "    value_match = re.search(r'Value:\\s*([\\d\\.]+)', text)\n",
    "    value = float(value_match.group(1)) if value_match else np.nan\n",
    "    unit_match = re.search(r'Unit:\\s*(\\w+)', text)\n",
    "    unit = unit_match.group(1) if unit_match else 'unknown'\n",
    "    return pd.Series([value, unit], index=['Value', 'Unit'])\n",
    "\n",
    "def standardize_units(unit):\n",
    "    if not isinstance(unit, str): return 'unknown'\n",
    "    unit = unit.lower().strip()\n",
    "    unit_map = {'oz': 'ounce', 'ounces': 'ounce', 'fl oz': 'fl_oz', 'fz': 'fl_oz', 'ct': 'count', 'none': 'unknown'}\n",
    "    return unit_map.get(unit, unit)\n",
    "\n",
    "# --- Load Saved Objects ---\n",
    "try:\n",
    "    PREPROCESSOR_PATH = 'processed_data/preprocessor.joblib'\n",
    "    VLM_VECTORIZER_PATH = 'processed_data/vlm_vectorizer.joblib'\n",
    "    MODEL_PATH = 'final_lgbm_model_for_submission.joblib'\n",
    "    \n",
    "    preprocessor = joblib.load(PREPROCESSOR_PATH)\n",
    "    vlm_vectorizer = joblib.load(VLM_VECTORIZER_PATH)\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    print(\"✅ All models and preprocessors loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading artifact: {e}\")\n",
    "    print(\"Please ensure you have run the full training pipeline on the complete dataset first.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Feature Engineering Function for New Data (using robust reindexing) ---\n",
    "def create_features_for_test_data(df, data_dir='processed_data'):\n",
    "    \"\"\"\n",
    "    Applies the complete feature engineering pipeline to new, unseen test data.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Applying Feature Engineering to Test Data ---\")\n",
    "    \n",
    "    df_indexed = df.set_index('sample_id')\n",
    "\n",
    "    print(\"Processing catalog content...\")\n",
    "    extracted_data = df['catalog_content'].apply(extract_structured_data)\n",
    "    df_features = pd.concat([df, extracted_data], axis=1)\n",
    "    df_features['cleaned_catalog'] = df_features['catalog_content'].apply(clean_text)\n",
    "    df_features['Unit_standardized'] = df_features['Unit'].apply(standardize_units)\n",
    "    X_text_struct = preprocessor.transform(df_features)\n",
    "\n",
    "    print(\"Processing VLM text features...\")\n",
    "    vlm_df_all = pd.read_csv(os.path.join(data_dir, 'smolvlm_extracted_features.csv')).set_index('sample_id')\n",
    "    vlm_df_aligned = vlm_df_all.reindex(df_indexed.index).fillna('')\n",
    "    vlm_df_aligned['cleaned_vlm'] = vlm_df_aligned['smolvlm_description'].apply(clean_text)\n",
    "    X_vlm = vlm_vectorizer.transform(vlm_df_aligned['cleaned_vlm'])\n",
    "\n",
    "    print(\"Processing visual features...\")\n",
    "    all_image_features = np.load(os.path.join(data_dir, 'X_image_features.npy'))\n",
    "    all_ids_df = pd.read_csv(os.path.join(data_dir, 'all_sample_ids.csv')).set_index('sample_id')\n",
    "    image_features_df = pd.DataFrame(all_image_features, index=all_ids_df.index)\n",
    "    image_features_aligned = image_features_df.reindex(df_indexed.index)\n",
    "    \n",
    "    if image_features_aligned.isnull().values.any():\n",
    "        print(\"Warning: Missing visual features for some IDs. Imputing with mean.\")\n",
    "        image_features_aligned = image_features_aligned.fillna(image_features_aligned.mean())\n",
    "    X_images_test = image_features_aligned.values\n",
    "\n",
    "    print(\"Combining all feature sets...\")\n",
    "    X_test_final = hstack([X_text_struct, X_vlm, X_images_test]).tocsr()\n",
    "    \n",
    "    if X_text_struct.shape[0] != X_images_test.shape[0]:\n",
    "         raise ValueError(f\"Row count mismatch! Text features: {X_text_struct.shape[0]}, Image features: {X_images_test.shape[0]}\")\n",
    "    \n",
    "    print(f\"✅ Final test feature matrix created with shape: {X_test_final.shape}\")\n",
    "    return X_test_final\n",
    "\n",
    "# --- 3. Main Prediction & Evaluation Block ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Define file paths for the sample data evaluation ---\n",
    "    # We will PREDICT on `sample_test.csv` and SCORE against `sample_test_out.csv`\n",
    "    TEST_DATA_PATH = '68e8d1d70b66d_student_resource\\student_resource\\dataset\\sample_test.csv'\n",
    "    GROUND_TRUTH_PATH = '68e8d1d70b66d_student_resource\\student_resource\\dataset\\sample_test_out.csv'\n",
    "    SUBMISSION_FILE_PATH = 'sample_test_prediction_output.csv' # Give it a different name\n",
    "    \n",
    "    print(f\"\\nLoading SAMPLE test data from: {TEST_DATA_PATH}\")\n",
    "    test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "    \n",
    "    # --- Create features for the test data ---\n",
    "    X_test = create_features_for_test_data(test_df.copy())\n",
    "    \n",
    "    # --- Make Predictions ---\n",
    "    print(\"\\nMaking final predictions on sample data...\")\n",
    "    predictions_log = model.predict(X_test)\n",
    "    \n",
    "    predictions_actual = np.expm1(predictions_log)\n",
    "    predictions_actual[predictions_actual < 0] = 0\n",
    "\n",
    "    # --- Create and Save the Prediction File ---\n",
    "    submission_df = pd.DataFrame({\n",
    "        'sample_id': test_df['sample_id'],\n",
    "        'price': predictions_actual\n",
    "    })\n",
    "    submission_df.to_csv(SUBMISSION_FILE_PATH, index=False)\n",
    "    print(f\"Prediction file for sample data generated at: '{SUBMISSION_FILE_PATH}'\")\n",
    "    \n",
    "    # --- NEW: Score the predictions against the ground truth ---\n",
    "    print(\"\\n--- Scoring Predictions Against Ground Truth ---\")\n",
    "    try:\n",
    "        ground_truth_df = pd.read_csv(GROUND_TRUTH_PATH)\n",
    "        \n",
    "        # Merge our predictions with the true prices\n",
    "        merged_score_df = pd.merge(submission_df, ground_truth_df, on='sample_id', suffixes=('_pred', '_true'))\n",
    "        \n",
    "        if len(merged_score_df) != len(test_df):\n",
    "            print(\"Warning: Not all sample IDs could be scored.\")\n",
    "\n",
    "        # Calculate and print the final scores\n",
    "        final_smape_score = smape(merged_score_df['price_true'], merged_score_df['price_pred'])\n",
    "        final_r2_score = r2_score(merged_score_df['price_true'], merged_score_df['price_pred'])\n",
    "\n",
    "        print(\"\\n--- FINAL PERFORMANCE ON SAMPLE DATA ---\")\n",
    "        print(f\"SMAPE Score: {final_smape_score:.4f}%\")\n",
    "        print(f\"R-squared (R²): {final_r2_score:.4f}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nCould not find ground truth file at '{GROUND_TRUTH_PATH}'. Skipping scoring.\")\n",
    "        \n",
    "    print(f\"\\n\\n🎉 SUCCESS! 🎉\")\n",
    "    print(\"The script has successfully generated predictions and scored them.\")\n",
    "    print(\"To generate the REAL submission, change TEST_DATA_PATH to 'dataset/test.csv' and remove the scoring section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59f3f4",
   "metadata": {},
   "source": [
    "extract_image_features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcac66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# GPU SETUP\n",
    "# ==========================================\n",
    "\n",
    "def setup_device():\n",
    "    \"\"\"Setup PyTorch device with optimization.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "        print(f\"✅ GPU: {gpu_name}\")\n",
    "        print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "        print(f\"   CUDA: {torch.version.cuda}\")\n",
    "\n",
    "        # Optimization for RTX 4060\n",
    "        torch.backends.cudnn.benchmark = True  # Auto-tune convolutions\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"⚠️  Using CPU\")\n",
    "\n",
    "    return device\n",
    "\n",
    "# ==========================================\n",
    "# MODEL SETUP\n",
    "# ==========================================\n",
    "\n",
    "def setup_model(device):\n",
    "    \"\"\"Load ResNet50 optimized for inference.\"\"\"\n",
    "    print(\"Loading ResNet50...\")\n",
    "\n",
    "    # Load pretrained model\n",
    "    model = models.resnet50(weights='IMAGENET1K_V2')\n",
    "\n",
    "    # Remove classification head (keep features only)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "    # Move to GPU and set to eval mode\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Enable mixed precision for faster inference (RTX 4060 supports this)\n",
    "    if device.type == 'cuda':\n",
    "        model = model.half()  # FP16 for 2x speedup\n",
    "        print(\"✅ Model loaded with FP16 optimization (2048D features)\")\n",
    "    else:\n",
    "        print(\"✅ Model loaded (2048D features)\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# ==========================================\n",
    "# IMAGE PREPROCESSING\n",
    "# ==========================================\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "def download_image(url, max_retries=3):\n",
    "    \"\"\"Download and preprocess image with retries.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                timeout=12,\n",
    "                headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Load and convert to RGB\n",
    "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "            # Apply transforms\n",
    "            return transform(img)\n",
    "\n",
    "        except Exception:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(0.3)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    return None\n",
    "\n",
    "# ==========================================\n",
    "# BATCH PROCESSING\n",
    "# ==========================================\n",
    "\n",
    "def process_batch(urls, sample_ids, model, device, max_workers=20):\n",
    "    \"\"\"Process batch with parallel downloads + GPU inference.\"\"\"\n",
    "\n",
    "    # Parallel download\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        images = list(executor.map(download_image, urls))\n",
    "\n",
    "    # Separate valid/invalid\n",
    "    valid_images = []\n",
    "    valid_indices = []\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        if img is not None:\n",
    "            valid_images.append(img)\n",
    "            valid_indices.append(i)\n",
    "\n",
    "    # Initialize with NaN\n",
    "    features = np.full((len(urls), 2048), np.nan, dtype=np.float32)\n",
    "\n",
    "    # GPU inference\n",
    "    if valid_images:\n",
    "        batch_tensor = torch.stack(valid_images).to(device)\n",
    "\n",
    "        # Use FP16 if GPU\n",
    "        if device.type == 'cuda':\n",
    "            batch_tensor = batch_tensor.half()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_features = model(batch_tensor)\n",
    "            batch_features = batch_features.squeeze(-1).squeeze(-1)\n",
    "            batch_features = batch_features.float().cpu().numpy()  # Back to FP32\n",
    "\n",
    "        # Assign features\n",
    "        for i, idx in enumerate(valid_indices):\n",
    "            features[idx] = batch_features[i]\n",
    "\n",
    "    success = [img is not None for img in images]\n",
    "\n",
    "    return sample_ids, features, success\n",
    "\n",
    "# ==========================================\n",
    "# CHECKPOINTING\n",
    "# ==========================================\n",
    "\n",
    "class Checkpoint:\n",
    "    \"\"\"Incremental saving with resume capability.\"\"\"\n",
    "\n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        self.features_file = os.path.join(output_dir, 'X_image_features.npy')\n",
    "        self.ids_file = os.path.join(output_dir, 'all_sample_ids.csv')\n",
    "\n",
    "    def load_progress(self):\n",
    "        \"\"\"Get already processed IDs.\"\"\"\n",
    "        if os.path.exists(self.ids_file):\n",
    "            df = pd.read_csv(self.ids_file)\n",
    "            processed = set(df['sample_id'].values)\n",
    "            print(f\"📂 Resuming: {len(processed)} processed\")\n",
    "            return processed\n",
    "        return set()\n",
    "\n",
    "    def save_batch(self, sample_ids, features):\n",
    "        \"\"\"Append batch.\"\"\"\n",
    "        # Features\n",
    "        if os.path.exists(self.features_file):\n",
    "            existing = np.load(self.features_file)\n",
    "            combined = np.vstack([existing, features])\n",
    "        else:\n",
    "            combined = features\n",
    "\n",
    "        np.save(self.features_file, combined)\n",
    "\n",
    "        # IDs\n",
    "        pd.DataFrame({'sample_id': sample_ids}).to_csv(\n",
    "            self.ids_file,\n",
    "            mode='a',\n",
    "            header=not os.path.exists(self.ids_file),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "# ==========================================\n",
    "# MAIN PIPELINE\n",
    "# ==========================================\n",
    "\n",
    "def extract_features(\n",
    "    train_csv='dataset/train.csv',\n",
    "    test_csv='dataset/test.csv',\n",
    "    output_dir='processed_data',\n",
    "    batch_size=96,\n",
    "    max_workers=20\n",
    "):\n",
    "    \"\"\"Main extraction pipeline.\"\"\"\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"Amazon ML Challenge 2025 - Image Feature Extraction\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Setup\n",
    "    device = setup_device()\n",
    "    model = setup_model(device)\n",
    "    checkpoint = Checkpoint(output_dir)\n",
    "\n",
    "    # Load data\n",
    "    print(f\"\\n📂 Loading data...\")\n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    df_test = pd.read_csv(test_csv)\n",
    "\n",
    "    df_all = pd.concat([\n",
    "        df_train[['sample_id', 'image_link']],\n",
    "        df_test[['sample_id', 'image_link']]\n",
    "    ]).drop_duplicates(subset=['sample_id']).reset_index(drop=True)\n",
    "\n",
    "    print(f\"   Total: {len(df_all)}\")\n",
    "\n",
    "    # Resume\n",
    "    processed = checkpoint.load_progress()\n",
    "    df_todo = df_all[~df_all['sample_id'].isin(processed)].reset_index(drop=True)\n",
    "\n",
    "    if df_todo.empty:\n",
    "        print(\"\\n✅ All processed!\")\n",
    "        return\n",
    "\n",
    "    print(f\"   Remaining: {len(df_todo)}\\n\")\n",
    "\n",
    "    # Process\n",
    "    num_batches = (len(df_todo) + batch_size - 1) // batch_size\n",
    "\n",
    "    print(f\"🚀 Processing batches of {batch_size} (ETA: ~{num_batches*2.5/60:.0f} min)\")\n",
    "    print(f\"   Workers: {max_workers}\\n\")\n",
    "\n",
    "    start = time.time()\n",
    "    stats = {'success': 0, 'failed': 0}\n",
    "\n",
    "    pbar = tqdm(range(0, len(df_todo), batch_size), total=num_batches, desc=\"Progress\")\n",
    "\n",
    "    for i in pbar:\n",
    "        batch_df = df_todo.iloc[i:i + batch_size]\n",
    "\n",
    "        batch_ids, batch_features, success = process_batch(\n",
    "            batch_df['image_link'].tolist(),\n",
    "            batch_df['sample_id'].tolist(),\n",
    "            model, device, max_workers\n",
    "        )\n",
    "\n",
    "        checkpoint.save_batch(batch_ids, batch_features)\n",
    "\n",
    "        # Stats\n",
    "        stats['success'] += sum(success)\n",
    "        stats['failed'] += len(success) - sum(success)\n",
    "\n",
    "        # Update progress bar\n",
    "        total = stats['success'] + stats['failed']\n",
    "        rate = stats['success'] / total * 100 if total > 0 else 0\n",
    "        speed = total / (time.time() - start)\n",
    "        pbar.set_postfix({\n",
    "            'Success': f\"{rate:.1f}%\",\n",
    "            'Speed': f\"{speed:.1f} img/s\"\n",
    "        })\n",
    "\n",
    "        # Clear GPU cache periodically\n",
    "        if i % 50 == 0 and device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"✅ Extraction Complete!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    total = stats['success'] + stats['failed']\n",
    "    print(f\"Total: {total:,}\")\n",
    "    print(f\"Success: {stats['success']:,} ({stats['success']/total*100:.1f}%)\")\n",
    "    print(f\"Failed: {stats['failed']:,}\")\n",
    "    print(f\"Time: {elapsed/60:.1f} min\")\n",
    "    print(f\"Speed: {total/elapsed:.1f} images/sec\")\n",
    "\n",
    "    # Impute\n",
    "    print(f\"\\n🔧 Final processing...\")\n",
    "    features = np.load(checkpoint.features_file)\n",
    "\n",
    "    if np.isnan(features).any():\n",
    "        n_missing = np.isnan(features).any(axis=1).sum()\n",
    "        print(f\"   Imputing {n_missing:,} samples...\")\n",
    "\n",
    "        col_means = np.nanmean(features, axis=0)\n",
    "        for col in range(features.shape[1]):\n",
    "            mask = np.isnan(features[:, col])\n",
    "            features[mask, col] = col_means[col]\n",
    "\n",
    "        np.save(checkpoint.features_file, features)\n",
    "\n",
    "    print(f\"\\n✅ Saved: {checkpoint.features_file}\")\n",
    "    print(f\"   Shape: {features.shape}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# RUN\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    CONFIG = {\n",
    "        'train_csv': 'dataset/train.csv',\n",
    "        'test_csv': 'dataset/test.csv',\n",
    "        'output_dir': 'processed_data',\n",
    "        'batch_size': 96,      # Optimized for RTX 4060\n",
    "        'max_workers': 20      # Max parallel downloads\n",
    "    }\n",
    "\n",
    "    extract_features(**CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna joblib scipy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706d895",
   "metadata": {},
   "source": [
    "dilkash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f62870",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm --config-settings=cmake.define.USE_GPU=ON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize tqdm for pandas apply functions\n",
    "tqdm.pandas(desc=\"Applying\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Configuration & Setup ---\n",
    "CONFIG = {\n",
    "    'n_folds': 5, # Start with 5 for faster iteration, can increase for final run\n",
    "    'random_state': 42,\n",
    "    'st_batch_size': 256, # Batch size for Sentence Transformer encoding\n",
    "}\n",
    "\n",
    "# --- GPU Check ---\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    print(f\"✅ GPU found: {torch.cuda.get_device_name(0)}. Training will be GPU-accelerated.\")\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print(\"⚠️ No GPU found. Training will run on CPU.\")\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords', quiet=True); nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# --- 2. Helper Functions (Advanced Regex and Feature Creation) ---\n",
    "lemmatizer = WordNetLemmatizer(); stop_words = set(stopwords.words('english'))\n",
    "# (All helper functions from your provided script are excellent and remain here)\n",
    "# ... [Keeping all your great helper functions: measurement_patterns, extract_from_text_with_patterns, etc.] ...\n",
    "def smape(y_true, y_pred):\n",
    "    y_true_actual = np.expm1(y_true); y_pred_actual = np.expm1(y_pred)\n",
    "    numerator = np.abs(y_pred_actual - y_true_actual); denominator = (np.abs(y_true_actual) + np.abs(y_pred_actual)) / 2\n",
    "    ratio = np.where(denominator == 0, 0, numerator / denominator); return np.mean(ratio) * 100\n",
    "measurement_patterns = {'weight': {'regex': r'(?:weight|net weight|gross weight|capacity|pack weight)\\s*[:\\-\\(\\[]?\\s*(\\d+\\.?\\d*)\\s*(kg|g|lbs|lb|oz|ounces|gram|kilogram)\\b', 'units': {'kg': 1000, 'g': 1, 'lbs': 453.592, 'lb': 453.592, 'oz': 28.3495, 'ounces': 28.3495, 'gram': 1, 'kilogram': 1000}, 'default_unit': 'g'}, 'volume': {'regex': r'(?:volume|liquid|capacity|ml|l|liter|fl oz|floz|fluid ounce)\\s*[:\\-\\(\\[]?\\s*(\\d+\\.?\\d*)\\s*(ml|l|liter|fl\\s*oz|floz|fluid\\s*ounce)\\b', 'units': {'ml': 1, 'l': 1000, 'liter': 1000, 'fl oz': 29.5735, 'floz': 29.5735, 'fluid ounce': 29.5735}, 'default_unit': 'ml'}, 'dimension': {'regex': r'(?:dimension|size|length|width|height|depth)\\s*[:\\-\\(\\[]?\\s*(\\d+\\.?\\d*)\\s*(cm|m|inch|in|ft|foot|meter|centimeter)\\b', 'units': {'cm': 1, 'm': 100, 'inch': 2.54, 'in': 2.54, 'ft': 30.48, 'foot': 30.48, 'meter': 100, 'centimeter': 1}, 'default_unit': 'cm'}, 'wattage': {'regex': r'(?:power|wattage|watts)\\s*[:\\-\\(\\[]?\\s*(\\d+\\.?\\d*)\\s*(w|watt|watts|kw|kilowatt)\\b', 'units': {'w': 1, 'watt': 1, 'watts': 1, 'kw': 1000, 'kilowatt': 1000}, 'default_unit': 'w'}, 'count': {'regex': r'(\\d+)\\s*(?:pack|pcs|pieces|units|ct|count)\\b', 'units': {'pack': 1, 'pcs': 1, 'pieces': 1, 'units': 1, 'ct': 1, 'count': 1}, 'default_unit': 'count'}, 'percentage': {'regex': r'(\\d+\\.?\\d*)\\s*%', 'units': {'%': 1}, 'default_unit': '%'}, 'value_and_unit': {'regex': r'value\\s*[:\\-\\(\\[]?\\s*([\\d\\.]+)\\s*unit\\s*[:\\-\\(\\[]?\\s*(\\w+)', 'units': {}, 'default_unit': 'unknown'}, 'general_number_unit': {'regex': r'(\\d+\\.?\\d*)\\s*([a-zA-Z]{1,5})\\b', 'units': {}, 'default_unit': 'unknown'}}\n",
    "def extract_from_text_with_patterns(text, pattern_type):\n",
    "    if not isinstance(text, str): return np.nan, 'unknown'\n",
    "    text_lower = text.lower(); pattern_info = measurement_patterns.get(pattern_type)\n",
    "    if not pattern_info: return np.nan, 'unknown'\n",
    "    regex = pattern_info['regex']; matches = re.findall(regex, text_lower)\n",
    "    if matches:\n",
    "        try: value = float(matches[0][0]); unit = matches[0][1]; return value, unit\n",
    "        except (ValueError, IndexError): return np.nan, 'unknown'\n",
    "    return np.nan, 'unknown'\n",
    "def extract_structured_data_advanced(text):\n",
    "    if not isinstance(text, str):\n",
    "        default_vals = {'extracted_value': np.nan, 'extracted_unit_value': np.nan, 'extracted_weight': np.nan, 'extracted_volume': np.nan, 'extracted_dimension': np.nan, 'extracted_wattage': np.nan, 'extracted_count': np.nan, 'extracted_percentage': np.nan, 'extracted_unit': 'unknown', 'extracted_weight_unit': 'unknown', 'extracted_volume_unit': 'unknown', 'extracted_dimension_unit': 'unknown', 'extracted_general_num_val': np.nan, 'extracted_general_num_unit': 'unknown'}\n",
    "        return pd.Series(default_vals)\n",
    "    text_lower = text.lower(); value, unit = extract_from_text_with_patterns(text, 'value_and_unit'); weight, weight_unit = extract_from_text_with_patterns(text, 'weight'); volume, volume_unit = extract_from_text_with_patterns(text, 'volume'); dimension, dimension_unit = extract_from_text_with_patterns(text, 'dimension'); wattage, wattage_unit = extract_from_text_with_patterns(text, 'wattage'); count, count_unit = extract_from_text_with_patterns(text, 'count'); percentage, percentage_unit = extract_from_text_with_patterns(text, 'percentage'); general_num_val, general_num_unit = extract_from_text_with_patterns(text, 'general_number_unit')\n",
    "    return pd.Series({'extracted_value': value, 'extracted_unit_value': float(re.search(r'(\\d+\\.?\\d*)', text_lower).group(1)) if re.search(r'(\\d+\\.?\\d*)', text_lower) else np.nan, 'extracted_weight': weight, 'extracted_volume': volume, 'extracted_dimension': dimension, 'extracted_wattage': wattage, 'extracted_count': count, 'extracted_percentage': percentage, 'extracted_unit': unit, 'extracted_weight_unit': weight_unit, 'extracted_volume_unit': volume_unit, 'extracted_dimension_unit': dimension_unit, 'extracted_general_num_val': general_num_val, 'extracted_general_num_unit': general_num_unit})\n",
    "def clean_text_improved(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'Value:\\s*[\\d\\.]+\\s*Unit:\\s*\\w+', ' ', text, flags=re.IGNORECASE); text = re.sub(r'Brand:.*?Product:', ' ', text, flags=re.IGNORECASE | re.DOTALL); text = re.sub(r'Size/Quantity:.*?Features:', ' ', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    text = re.sub(r'\\b\\d+\\.?\\d*\\s*(?:kg|g|lbs?|oz|ml|l|liter|fl\\s?oz|floz|cm|m|inch|in|ft|foot|w|watt|watts|kw|pack|pcs|pieces|units|ct|count|%)?\\b', ' ', text, flags=re.IGNORECASE); text = re.sub(r'\\b(?:xl|l|m|s|xs|large|medium|small|extra large|extra small)\\b', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'<.*?>', ' ', text); text = re.sub(r'[^a-zA-Z\\s]', ' ', text).lower(); words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words and len(w) > 1]; return ' '.join(lemmatized_words)\n",
    "def create_better_features(df):\n",
    "    features = pd.DataFrame(index=df.index); features['catalog_length'] = df['catalog_content'].str.len().fillna(0)\n",
    "    if 'smolvlm_description' in df.columns: features['vlm_length'] = df['smolvlm_description'].str.len().fillna(0)\n",
    "    features['catalog_word_count'] = df['catalog_content'].str.split().str.len().fillna(0)\n",
    "    if 'smolvlm_description' in df.columns: features['vlm_word_count'] = df['smolvlm_description'].str.split().str.len().fillna(0)\n",
    "    features['catalog_num_count'] = df['catalog_content'].apply(lambda x: len(re.findall(r'\\d+\\.?\\d*', str(x)))).fillna(0)\n",
    "    if 'smolvlm_description' in df.columns: features['vlm_num_count'] = df['smolvlm_description'].apply(lambda x: len(re.findall(r'\\d+\\.?\\d*', str(x)))).fillna(0)\n",
    "    features['catalog_numeric_ratio'] = df['catalog_content'].astype(str).apply(lambda x: sum(c.isdigit() for c in x) / len(x) if len(x) > 0 else 0)\n",
    "    if 'smolvlm_description' in df.columns: features['vlm_numeric_ratio'] = df['smolvlm_description'].astype(str).apply(lambda x: sum(c.isdigit() for c in x) / len(x) if len(x) > 0 else 0)\n",
    "    keywords = ['electronic', 'furniture', 'apparel', 'food', 'book', 'toy', 'tool', 'home', 'garden', 'kitchen', 'beauty', 'health', 'automotive']\n",
    "    for kw in keywords:\n",
    "        features[f'has_kw_{kw}'] = df['cleaned_catalog'].apply(lambda x: 1 if kw in x else 0)\n",
    "        if 'cleaned_vlm' in df.columns: features[f'vlm_has_kw_{kw}'] = df['cleaned_vlm'].apply(lambda x: 1 if kw in x else 0)\n",
    "    if 'has_vlm_data' in df.columns and 'has_kw_electronic' in features.columns: features['vlm_electronic_interaction'] = df['has_vlm_data'] * features['has_kw_electronic']\n",
    "    features['has_extracted_weight'] = df['extracted_weight'].notna().astype(int); features['has_extracted_volume'] = df['extracted_volume'].notna().astype(int); features['has_extracted_dimension'] = df['extracted_dimension'].notna().astype(int); features['has_extracted_wattage'] = df['extracted_wattage'].notna().astype(int); features['has_extracted_count'] = df['extracted_count'].notna().astype(int); features['has_extracted_percentage'] = df['extracted_percentage'].notna().astype(int)\n",
    "    return features\n",
    "\n",
    "# --- 3. Main Feature Engineering and Training Pipeline ---\n",
    "def run_complete_pipeline():\n",
    "    \"\"\"Main pipeline to load, feature engineer, and train models on the full dataset.\"\"\"\n",
    "    OUTPUT_DIR = 'processed_data'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # --- Load FULL training data ---\n",
    "    try:\n",
    "        print(\"Loading full training data...\"); train_df = pd.read_csv(os.path.join('dataset', 'train.csv'))\n",
    "        print(f\"✅ Loaded {len(train_df)} training samples.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: dataset/train.csv not found!\"); return\n",
    "\n",
    "    # --- VLM Feature Loading ---\n",
    "    try:\n",
    "        print(\"Loading VLM features...\"); vlm_df = pd.read_csv(os.path.join(OUTPUT_DIR, 'smolvlm_extracted_features.csv'))\n",
    "        merged_df = train_df.merge(vlm_df, on='sample_id', how='left')\n",
    "        merged_df['has_vlm_data'] = ~merged_df['smolvlm_description'].isna() & (merged_df['smolvlm_description'].str.strip() != '')\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠️ VLM features not found, proceeding without them.\"); merged_df = train_df.copy()\n",
    "        merged_df['has_vlm_data'] = False; merged_df['smolvlm_description'] = \"\"\n",
    "    \n",
    "    features_df = merged_df.copy()\n",
    "    \n",
    "    # --- On-the-fly Feature Engineering with Progress Bars ---\n",
    "    print(\"\\n--- Starting Feature Engineering on Full Dataset ---\")\n",
    "    print(\"1. Performing advanced structured data extraction...\"); extracted_data = features_df['catalog_content'].progress_apply(extract_structured_data_advanced)\n",
    "    features_df = pd.concat([features_df.reset_index(drop=True), extracted_data], axis=1)\n",
    "    \n",
    "    print(\"2. Cleaning text features...\"); features_df['cleaned_catalog'] = features_df['catalog_content'].progress_apply(clean_text_improved)\n",
    "    features_df['cleaned_vlm'] = features_df['smolvlm_description'].progress_apply(clean_text_improved)\n",
    "    \n",
    "    print(\"3. Creating meta-features...\"); additional_features = create_better_features(features_df)\n",
    "    features_df = pd.concat([features_df, additional_features], axis=1)\n",
    "    \n",
    "    features_df['combined_text'] = features_df.apply(\n",
    "        lambda x: x['cleaned_vlm'] if x['has_vlm_data'] else x['cleaned_catalog'], axis=1\n",
    "    )\n",
    "    \n",
    "    # --- Sentence Transformer Embeddings ---\n",
    "    print(\"4. Generating Sentence Transformer embeddings...\")\n",
    "    st_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
    "    catalog_embeddings = st_model.encode(features_df['cleaned_catalog'].tolist(), batch_size=CONFIG['st_batch_size'], show_progress_bar=True)\n",
    "    vlm_embeddings = st_model.encode(features_df['cleaned_vlm'].tolist(), batch_size=CONFIG['st_batch_size'], show_progress_bar=True)\n",
    "    \n",
    "    # --- Prepare for Preprocessing ---\n",
    "    y = np.log1p(features_df['price'].values)\n",
    "    cols_to_drop = ['price', 'sample_id', 'image_link', 'catalog_content', 'smolvlm_description', 'cleaned_catalog', 'cleaned_vlm']\n",
    "    features_df.drop(columns=[c for c in cols_to_drop if c in features_df.columns], inplace=True)\n",
    "    \n",
    "    # --- Build Preprocessing Pipeline ---\n",
    "    print(\"5. Building preprocessing pipeline...\");\n",
    "    numeric_features = [c for c in features_df.columns if features_df[c].dtype in ['int64', 'float64', 'bool']]\n",
    "    categorical_features = [c for c in features_df.columns if features_df[c].dtype == 'object' and c != 'combined_text']\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_features),\n",
    "            ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='unknown')), ('onehot', OneHotEncoder(handle_unknown='ignore', max_categories=50))]), categorical_features),\n",
    "            ('text', TfidfVectorizer(max_features=3000, ngram_range=(1, 2), min_df=5, max_df=0.7), 'combined_text')\n",
    "        ], remainder='drop', n_jobs=-1)\n",
    "    \n",
    "    print(\"6. Fitting preprocessor and transforming data...\");\n",
    "    X_base = preprocessor.fit_transform(features_df)\n",
    "    \n",
    "    # --- Final Combination of ALL Features ---\n",
    "    X_final = hstack([X_base, catalog_embeddings, vlm_embeddings]).tocsr()\n",
    "    print(f\"   ✓ Final training data shape: {X_final.shape}\")\n",
    "\n",
    "    # --- Train Ensemble with Cross-Validation ---\n",
    "    print(\"\\n\" + \"=\"*50); print(\"🚀 Starting GPU-Accelerated Ensemble Training 🚀\"); print(f\"   Folds: {CONFIG['n_folds']}, Device: {DEVICE.upper()}\"); print(\"=\"*50)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['random_state'])\n",
    "    lgb_params = {'objective': 'regression_l1', 'metric': 'mae', 'random_state': CONFIG['random_state'], 'n_estimators': 4000, 'learning_rate': 0.01, 'num_leaves': 50, 'subsample': 0.7, 'colsample_bytree': 0.7, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'n_jobs': -1, 'device': DEVICE}\n",
    "    xgb_params = {'objective': 'reg:squarederror', 'eval_metric': 'mae', 'seed': CONFIG['random_state'], 'n_estimators': 4000, 'learning_rate': 0.01, 'max_depth': 8, 'subsample': 0.7, 'colsample_bytree': 0.7, 'tree_method': 'hist', 'device': DEVICE, 'early_stopping_rounds': 200, 'n_jobs': -1}\n",
    "    cat_params = {'loss_function': 'MAE', 'eval_metric': 'MAE', 'random_seed': CONFIG['random_state'], 'iterations': 4000, 'learning_rate': 0.05, 'depth': 8, 'verbose': 0, 'task_type': 'GPU' if DEVICE == 'cuda' else 'CPU'}\n",
    "\n",
    "    model_configs = {\n",
    "        'lgbm': {'model': lgb.LGBMRegressor(**lgb_params), 'preds': np.zeros(len(y))},\n",
    "        'xgb': {'model': xgb.XGBRegressor(**xgb_params), 'preds': np.zeros(len(y))},\n",
    "        'cat': {'model': cb.CatBoostRegressor(**cat_params), 'preds': np.zeros(len(y))}\n",
    "    }\n",
    "    oof_scores = {name: [] for name in model_configs}\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(X_final, y), total=CONFIG['n_folds'], desc=\"Overall CV Progress\")):\n",
    "        X_train, X_val = X_final[train_idx], X_final[val_idx]; y_train, y_val = y[train_idx], y[val_idx]\n",
    "        for name, config in model_configs.items():\n",
    "            print(f\"  - Training {name} on Fold {fold+1}...\"); start_time = time.time()\n",
    "            try:\n",
    "                if name == 'lgbm': config['model'].fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(200, verbose=False)])\n",
    "                elif name == 'xgb': config['model'].fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "                else: config['model'].fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=200, verbose=False)\n",
    "            except Exception as e:\n",
    "                if name == 'lgbm' and \"CUDA\" in str(e): print(\"    ⚠️ LGBM GPU failed, falling back to CPU...\"); cpu_params = lgb_params.copy(); del cpu_params['device']; config['model'] = lgb.LGBMRegressor(**cpu_params); config['model'].fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(200, verbose=False)])\n",
    "                else: print(f\"    ⚠️ Error training {name}: {e}. Skipping.\"); continue\n",
    "            \n",
    "            val_preds = config['model'].predict(X_val); config['preds'][val_idx] = val_preds\n",
    "            fold_smape = smape(y, val_preds); oof_scores[name].append(fold_smape); elapsed = time.time() - start_time\n",
    "            print(f\"    ✓ {name} Fold {fold+1} SMAPE: {fold_smape:.4f}% ({elapsed:.1f}s)\")\n",
    "            \n",
    "    print(\"\\n\" + \"=\"*50); print(\"📊 Overall Cross-Validation Results (OOF)\"); print(\"=\"*50)\n",
    "    for name, scores in oof_scores.items():\n",
    "        if scores: print(f\"{name.upper():<6} | Mean SMAPE: {np.mean(scores):.4f}% (Std: {np.std(scores):.4f})\")\n",
    "    \n",
    "    ensemble_preds = (0.5 * model_configs['lgbm']['preds'] + 0.3 * model_configs['xgb']['preds'] + 0.2 * model_configs['cat']['preds'])\n",
    "    ensemble_smape = smape(y, ensemble_preds); print(f\"\\n{'ENSEMBLE':<6} | Final OOF SMAPE: {ensemble_smape:.4f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50); print(\"Retraining final LGBM model on all data...\"); print(\"=\"*50)\n",
    "    final_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    try: final_model.fit(X_final, y)\n",
    "    except Exception: print(\"   ⚠️ GPU failed for final model, retraining on CPU...\"); cpu_params = lgb_params.copy(); del cpu_params['device']; final_model = lgb.LGBMRegressor(**cpu_params); final_model.fit(X_final, y)\n",
    "            \n",
    "    joblib.dump(final_model, 'final_ensemble_model_for_submission.joblib')\n",
    "    joblib.dump(preprocessor, os.path.join(OUTPUT_DIR, 'preprocessor_final.joblib'))\n",
    "    print(\"\\n✅ Final model and preprocessor saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_complete_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6669d8e",
   "metadata": {},
   "source": [
    "sentence tranformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762659f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this file as: preprocess_final.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.sparse import save_npz, hstack\n",
    "import joblib\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# --- GPU Check ---\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    print(f\"✅ GPU found: {torch.cuda.get_device_name(0)}. Embeddings will be generated on GPU.\")\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print(\"⚠️ No GPU found. Embeddings will be generated on CPU.\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# (Functions clean_text, extract_structured_data, standardize_units are correct and remain the same)\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'Value:.*', '', text, flags=re.IGNORECASE); text = re.sub(r'Unit:.*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'<.*?>', ' ', text); text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    words = text.split()\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in words if w not in stop_words and len(w) > 1]\n",
    "    return ' '.join(lemmatized)\n",
    "def extract_structured_data(text):\n",
    "    if not isinstance(text, str): return pd.Series([np.nan, 'unknown'], index=['Value', 'Unit'])\n",
    "    value_match = re.search(r'Value:\\s*([\\d\\.]+)', text, re.IGNORECASE); value = float(value_match.group(1)) if value_match else np.nan\n",
    "    unit_match = re.search(r'Unit:\\s*(\\w+)', text, re.IGNORECASE); unit = unit_match.group(1) if unit_match else 'unknown'\n",
    "    return pd.Series([value, unit], index=['Value', 'Unit'])\n",
    "def standardize_units(unit):\n",
    "    if not isinstance(unit, str): return 'unknown'\n",
    "    unit = unit.lower().strip(); unit_map = {'oz': 'ounce', 'ounces': 'ounce', 'fl oz': 'fl_oz', 'fz': 'fl_oz', 'ct': 'count', 'none': 'unknown'}\n",
    "    return unit_map.get(unit, unit)\n",
    "\n",
    "# --- Main Preprocessing Pipeline ---\n",
    "def create_all_features(df, is_training=True, output_dir='processed_data'):\n",
    "    \n",
    "    print(\"\\nExtracting structured data from catalog...\")\n",
    "    extracted_data = df['catalog_content'].apply(extract_structured_data)\n",
    "    df = pd.concat([df.reset_index(drop=True), extracted_data], axis=1)\n",
    "\n",
    "    # --- Setup common DataFrame for features ---\n",
    "    if is_training:\n",
    "        df.dropna(subset=['price'], inplace=True)\n",
    "        y = np.log1p(df['price'])\n",
    "        sample_ids = df['sample_id']\n",
    "        features_df = df.drop(columns=['price', 'sample_id', 'image_link'])\n",
    "    else: # is_training == False\n",
    "        sample_ids = df['sample_id']\n",
    "        features_df = df.drop(columns=['sample_id', 'image_link'])\n",
    "        if 'price' in features_df.columns: features_df = features_df.drop(columns=['price'])\n",
    "\n",
    "    print(\"Cleaning all text sources (Catalog and VLM)...\")\n",
    "    features_df['cleaned_catalog'] = features_df['catalog_content'].apply(clean_text)\n",
    "    features_df['Unit_standardized'] = features_df['Unit'].apply(standardize_units)\n",
    "    \n",
    "    # --- 1. Base TF-IDF and Scalers ---\n",
    "    if is_training:\n",
    "        print(\"1. Fitting base preprocessor (TF-IDF, Scalers)...\")\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), ['Value']),\n",
    "                ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='unknown')), ('onehot', OneHotEncoder(handle_unknown='ignore'))]), ['Unit_standardized']),\n",
    "                ('text', TfidfVectorizer(max_features=5000, ngram_range=(1, 2)), 'cleaned_catalog')\n",
    "            ], remainder='drop', n_jobs=-1)\n",
    "        X_base = preprocessor.fit_transform(features_df)\n",
    "        joblib.dump(preprocessor, os.path.join(output_dir, 'preprocessor.joblib'))\n",
    "    else:\n",
    "        print(\"1. Loading base preprocessor and transforming test data...\")\n",
    "        preprocessor = joblib.load(os.path.join(output_dir, 'preprocessor.joblib'))\n",
    "        X_base = preprocessor.transform(features_df)\n",
    "    \n",
    "    # --- 2. Sentence Transformer Embeddings for Catalog ---\n",
    "    print(\"\\n2. Generating Sentence Transformer embeddings for Catalog...\")\n",
    "    st_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
    "    catalog_embeddings = st_model.encode(\n",
    "        features_df['cleaned_catalog'].tolist(), batch_size=256, show_progress_bar=True, convert_to_numpy=True\n",
    "    )\n",
    "\n",
    "    # --- 3. VLM Text Processing (TF-IDF and Embeddings) ---\n",
    "    print(\"\\n3. Processing VLM text (TF-IDF and Embeddings)...\")\n",
    "    vlm_file = 'smolvlm_extracted_features[1].csv' # Hardcoded to your filename\n",
    "    vlm_df = pd.read_csv(os.path.join(output_dir, vlm_file)).set_index('sample_id')\n",
    "    vlm_df_aligned = vlm_df.reindex(sample_ids).fillna('')\n",
    "    vlm_df_aligned['cleaned_vlm'] = vlm_df_aligned['smolvlm_description'].apply(clean_text)\n",
    "    \n",
    "    if is_training:\n",
    "        vlm_vectorizer = TfidfVectorizer(max_features=1500, ngram_range=(1, 2))\n",
    "        X_vlm_tfidf = vlm_vectorizer.fit_transform(vlm_df_aligned['cleaned_vlm'])\n",
    "        joblib.dump(vlm_vectorizer, os.path.join(output_dir, 'vlm_vectorizer.joblib'))\n",
    "    else:\n",
    "        vlm_vectorizer = joblib.load(os.path.join(output_dir, 'vlm_vectorizer.joblib'))\n",
    "        X_vlm_tfidf = vlm_vectorizer.transform(vlm_df_aligned['cleaned_vlm'])\n",
    "\n",
    "    vlm_embeddings = st_model.encode(\n",
    "        vlm_df_aligned['cleaned_vlm'].tolist(), batch_size=256, show_progress_bar=True, convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    # --- 4. Visual (Image) Feature Processing ---\n",
    "    print(\"\\n4. Processing visual (image) features...\")\n",
    "    all_image_features = np.load(os.path.join(output_dir, 'X_image_features.npy'))\n",
    "    all_ids_df = pd.read_csv(os.path.join(output_dir, 'all_sample_ids.csv'))\n",
    "    image_features_df = pd.DataFrame(all_image_features, index=all_ids_df['sample_id'])\n",
    "    X_images_aligned = image_features_df.reindex(sample_ids).values\n",
    "    \n",
    "    if is_training:\n",
    "        image_scaler = StandardScaler()\n",
    "        X_images_scaled = image_scaler.fit_transform(X_images_aligned)\n",
    "        joblib.dump(image_scaler, os.path.join(output_dir, 'image_scaler.joblib'))\n",
    "    else:\n",
    "        image_scaler = joblib.load(os.path.join(output_dir, 'image_scaler.joblib'))\n",
    "        X_images_scaled = image_scaler.transform(X_images_aligned)\n",
    "        \n",
    "    # --- 5. Combine and Save All Features ---\n",
    "    print(\"\\n5. Combining and saving all feature sets...\")\n",
    "    X_final = hstack([X_base, catalog_embeddings, X_vlm_tfidf, vlm_embeddings, X_images_scaled]).tocsr()\n",
    "    \n",
    "    # Save the final combined matrix\n",
    "    mode_str = 'train' if is_training else 'test'\n",
    "    save_npz(os.path.join(output_dir, f'X_final_{mode_str}.npz'), X_final)\n",
    "\n",
    "    # Save corresponding labels or IDs\n",
    "    if is_training:\n",
    "        y_df = pd.DataFrame({'sample_id': sample_ids, 'price_log': y})\n",
    "        y_df.to_csv(os.path.join(output_dir, f'y_{mode_str}.csv'), index=False)\n",
    "    else:\n",
    "        ids_df = pd.DataFrame({'sample_id': sample_ids})\n",
    "        ids_df.to_csv(os.path.join(output_dir, f'ids_{mode_str}.csv'), index=False)\n",
    "        \n",
    "    print(f\"\\n✅ All artifacts for '{mode_str.upper()}' mode are saved. Final shape: {X_final.shape}\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == '__main__':\n",
    "    MODE = 'train' # or 'test'\n",
    "    \n",
    "    if MODE == 'train':\n",
    "        train_df = pd.read_csv(os.path.join('dataset', 'train.csv'))\n",
    "        create_all_features(train_df, is_training=True)\n",
    "    elif MODE == 'test':\n",
    "        test_df = pd.read_csv(os.path.join('dataset', 'test.csv'))\n",
    "        create_all_features(test_df, is_training=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c627772",
   "metadata": {},
   "source": [
    "sentence tranformer training kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96194189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz, hstack\n",
    "import joblib\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Configuration & Setup ---\n",
    "CONFIG = {\n",
    "    'n_folds': 5, # Start with 5 folds for faster iteration, can increase to 20 later\n",
    "    'random_state': 42,\n",
    "    'data_dir': 'processed_data',\n",
    "}\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- 2. Helper Functions ---\n",
    "def smape(y_true, y_pred):\n",
    "    y_true_actual = np.expm1(y_true)\n",
    "    y_pred_actual = np.expm1(y_pred)\n",
    "    numerator = np.abs(y_pred_actual - y_true_actual)\n",
    "    denominator = (np.abs(y_true_actual) + np.abs(y_pred_actual)) / 2\n",
    "    ratio = np.where(denominator == 0, 0, numerator / denominator)\n",
    "    return np.mean(ratio) * 100\n",
    "\n",
    "# --- 3. Main Training and Ensembling Pipeline ---\n",
    "def train_ensemble(X, y):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🚀 Starting GPU-Accelerated Ensemble Training 🚀\")\n",
    "    print(f\"   Folds: {CONFIG['n_folds']}, Device: {DEVICE.upper()}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['random_state'])\n",
    "    \n",
    "    # Base Hyperparameters with GPU settings\n",
    "    lgb_params = {'objective': 'regression_l1', 'metric': 'mae', 'random_state': CONFIG['random_state'], 'n_estimators': 4000, 'learning_rate': 0.01, 'num_leaves': 50, 'subsample': 0.7, 'colsample_bytree': 0.7, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'n_jobs': -1, 'device': DEVICE}\n",
    "    xgb_params = {'objective': 'reg:squarederror', 'eval_metric': 'mae', 'seed': CONFIG['random_state'], 'n_estimators': 4000, 'learning_rate': 0.01, 'max_depth': 8, 'subsample': 0.7, 'colsample_bytree': 0.7, 'tree_method': 'hist', 'device': DEVICE, 'early_stopping_rounds': 200, 'n_jobs': -1}\n",
    "    cat_params = {'loss_function': 'MAE', 'eval_metric': 'MAE', 'random_seed': CONFIG['random_state'], 'iterations': 4000, 'learning_rate': 0.05, 'depth': 8, 'task_type': 'GPU' if DEVICE == 'cuda' else 'CPU'}\n",
    "\n",
    "    model_configs = {\n",
    "        'lgbm': {'model': lgb.LGBMRegressor(**lgb_params), 'preds': np.zeros(len(y))},\n",
    "        'xgb': {'model': xgb.XGBRegressor(**xgb_params), 'preds': np.zeros(len(y))},\n",
    "        'cat': {'model': cb.CatBoostRegressor(**cat_params), 'preds': np.zeros(len(y))}\n",
    "    }\n",
    "    oof_scores = {name: [] for name in model_configs}\n",
    "\n",
    "    # Wrap the KFold loop with tqdm for a master progress bar\n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(X, y), total=CONFIG['n_folds'], desc=\"Overall CV Progress\")):\n",
    "        print(f\"\\n--- Fold {fold+1}/{CONFIG['n_folds']} ---\")\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        for name, config in model_configs.items():\n",
    "            print(f\"  - Training {name}...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Added verbose logging to all training calls\n",
    "                if name == 'lgbm':\n",
    "                    config['model'].fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                                        callbacks=[lgb.early_stopping(200, verbose=False), lgb.log_evaluation(period=500)])\n",
    "                elif name == 'xgb':\n",
    "                    config['model'].fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=500)\n",
    "                else: # CatBoost\n",
    "                    config['model'].fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=200, verbose=500)\n",
    "            \n",
    "            except Exception as e:\n",
    "                if name == 'lgbm' and \"CUDA Tree Learner was not enabled\" in str(e):\n",
    "                    print(\"    ⚠️ LightGBM GPU failed. Falling back to CPU (this will be slow)...\")\n",
    "                    cpu_params = lgb_params.copy(); del cpu_params['device']\n",
    "                    config['model'] = lgb.LGBMRegressor(**cpu_params)\n",
    "                    config['model'].fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                                        callbacks=[lgb.early_stopping(200, verbose=False), lgb.log_evaluation(period=500)])\n",
    "                else:\n",
    "                    print(f\"    ⚠️ An error occurred training {name}: {e}. Skipping model for this fold.\")\n",
    "                    oof_scores[name].append(np.inf) # Add a high score to indicate failure\n",
    "                    continue\n",
    "\n",
    "            val_preds = config['model'].predict(X_val)\n",
    "            config['preds'][val_idx] = val_preds\n",
    "            fold_smape = smape(y_val, val_preds)\n",
    "            oof_scores[name].append(fold_smape)\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"    ✓ Done in {elapsed:.1f}s. Fold SMAPE: {fold_smape:.4f}%\")\n",
    "            \n",
    "    # --- Final Evaluation ---\n",
    "    print(\"\\n\" + \"=\"*50); print(\"📊 Overall Cross-Validation Results (OOF)\"); print(\"=\"*50)\n",
    "    for name, scores in oof_scores.items():\n",
    "        if scores:\n",
    "            print(f\"{name.upper():<6} | Mean SMAPE: {np.mean(scores):.4f}% (Std: {np.std(scores):.4f})\")\n",
    "    \n",
    "    ensemble_preds = (0.5 * model_configs['lgbm']['preds'] + \n",
    "                      0.3 * model_configs['xgb']['preds'] + \n",
    "                      0.2 * model_configs['cat']['preds'])\n",
    "    ensemble_smape = smape(y, ensemble_preds)\n",
    "    print(f\"\\n{'ENSEMBLE':<6} | Final OOF SMAPE: {ensemble_smape:.4f}% (Weighted Average)\")\n",
    "    \n",
    "    # --- Retrain Final Model ---\n",
    "    print(\"\\n\" + \"=\"*50); print(\"Retraining best model (LGBM) on 100% of data for submission...\"); print(\"=\"*50)\n",
    "    try:\n",
    "        final_model = lgb.LGBMRegressor(**lgb_params)\n",
    "        final_model.fit(X, y)\n",
    "    except Exception as e:\n",
    "        if \"CUDA Tree Learner was not enabled\" in str(e):\n",
    "            print(\"    ⚠️ GPU failed for final model. Retraining on CPU.\")\n",
    "            cpu_params = lgb_params.copy(); del cpu_params['device']\n",
    "            final_model = lgb.LGBMRegressor(**cpu_params)\n",
    "            final_model.fit(X, y)\n",
    "        else: raise e\n",
    "            \n",
    "    joblib.dump(final_model, 'final_ensemble_model_for_submission.joblib')\n",
    "    print(\"\\n✅ Final retrained model saved as 'final_ensemble_model_for_submission.joblib'\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == '__main__':\n",
    "    # This assumes your full feature matrix has been saved by a master preprocessing script\n",
    "    def load_precomputed_data(data_dir):\n",
    "        try:\n",
    "            print(\"--- Loading final precomputed feature matrix for training ---\")\n",
    "            X = load_npz(os.path.join(data_dir, 'X_final_train.npz'))\n",
    "            y_df = pd.read_csv(os.path.join(data_dir, 'y_train.csv'))\n",
    "            print(f\"   ✓ Features loaded. Shape: {X.shape}\")\n",
    "            print(f\"   ✓ Labels loaded. Count: {len(y_df)}\")\n",
    "            return X, y_df['price_log'].values\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"FATAL ERROR: Could not load precomputed files. {e}\")\n",
    "            print(\"Please run the final, combined preprocessing script first to generate 'X_final_train.npz' and 'y_train.csv'.\")\n",
    "            return None, None\n",
    "\n",
    "    X_final_train, y_final_train = load_precomputed_data(CONFIG['data_dir'])\n",
    "    \n",
    "    if X_final_train is not None:\n",
    "        train_ensemble(X_final_train, y_final_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e5d88",
   "metadata": {},
   "source": [
    "check for connected slaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f7d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dask.distributed import Client, TimeoutError\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use the same Dask scheduler address as your main script\n",
    "DASK_ADDRESS = \"tcp://10.0.22.229:8790\"\n",
    "\n",
    "def check_worker_status():\n",
    "    \"\"\"\n",
    "    Connects to the Dask scheduler and prints the status of all connected workers.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Attempting to connect to Dask scheduler at: {DASK_ADDRESS}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        # Connect to the scheduler with a timeout to avoid hanging indefinitely\n",
    "        client = Client(DASK_ADDRESS, timeout=\"10s\")\n",
    "    except (TimeoutError, OSError) as e:\n",
    "        print(f\"\\n❌ FAILED TO CONNECT TO SCHEDULER.\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        print(\"\\n   Troubleshooting Steps:\")\n",
    "        print(\"   1. Is the Dask scheduler running at the specified address?\")\n",
    "        print(\"   2. Is the address and port correct?\")\n",
    "        print(\"   3. Is there a firewall blocking the connection between this machine and the scheduler?\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Get information about the cluster\n",
    "        scheduler_info = client.scheduler_info()\n",
    "        workers = scheduler_info.get('workers', {})\n",
    "\n",
    "        if not workers:\n",
    "            print(\"\\n🟡 STATUS: Connected to scheduler, but NO workers are registered.\")\n",
    "            print(\"   Please ensure your dask-worker processes are running and pointing to the correct scheduler address.\")\n",
    "        else:\n",
    "            print(f\"\\n✅ SUCCESS: Connected to scheduler. Found {len(workers)} worker(s).\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            # Prepare data for a clean table display\n",
    "            worker_data = []\n",
    "            for address, info in workers.items():\n",
    "                worker_data.append({\n",
    "                    \"Address\": address,\n",
    "                    \"Status\": info.get('status', 'N/A'),\n",
    "                    \"Cores\": info.get('nthreads', 'N/A'),\n",
    "                    \"Memory Limit\": info.get('memory_limit', 0) / (1024**3), # Convert bytes to GB\n",
    "                    \"Hostname\": info.get('host', 'N/A')\n",
    "                })\n",
    "            \n",
    "            # Display as a formatted table using pandas\n",
    "            df = pd.DataFrame(worker_data)\n",
    "            df['Memory Limit'] = df['Memory Limit'].map('{:,.2f} GB'.format)\n",
    "            print(df.to_string(index=False))\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An error occurred while fetching worker information: {e}\")\n",
    "    finally:\n",
    "        # Always close the client connection\n",
    "        client.close()\n",
    "        print(\"\\nClient disconnected.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_worker_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a4718",
   "metadata": {},
   "source": [
    "training code for lightgbm using gpu only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "import joblib\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Configuration & Setup ---\n",
    "CONFIG = {\n",
    "    'n_folds': 20,  # As requested, set to 20 folds\n",
    "    'random_state': 42,\n",
    "    'data_dir': 'processed_data',\n",
    "}\n",
    "\n",
    "# Enforce GPU usage and print device info\n",
    "try:\n",
    "    assert torch.cuda.is_available()\n",
    "    DEVICE = 'cuda'\n",
    "    print(f\"✅ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "except AssertionError:\n",
    "    raise RuntimeError(\"❌ GPU NOT AVAILABLE! This script requires a CUDA-enabled GPU to run.\")\n",
    "\n",
    "\n",
    "# --- 2. Helper Function (SMAPE Metric) ---\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Calculates SMAPE score on the log-transformed predictions.\"\"\"\n",
    "    y_true_actual = np.expm1(y_true)\n",
    "    y_pred_actual = np.expm1(y_pred)\n",
    "    \n",
    "    # To prevent division by zero, especially if both true and pred are 0\n",
    "    numerator = np.abs(y_pred_actual - y_true_actual)\n",
    "    denominator = (np.abs(y_true_actual) + np.abs(y_pred_actual)) / 2\n",
    "    \n",
    "    ratio = np.where(denominator == 0, 0, numerator / denominator)\n",
    "    return np.mean(ratio) * 100\n",
    "\n",
    "\n",
    "# --- 3. Main Training Pipeline ---\n",
    "def train_lgbm_with_checkpoints(X, y):\n",
    "    \"\"\"\n",
    "    Trains a LightGBM model using 20-fold CV on a GPU.\n",
    "    Saves a checkpoint of the best model after each fold.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🚀 Starting LightGBM GPU Training with 20-Fold CV 🚀\")\n",
    "    print(f\"   Folds: {CONFIG['n_folds']}, Device: {DEVICE.upper()}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['random_state'])\n",
    "    \n",
    "    # LightGBM Hyperparameters optimized for GPU memory efficiency\n",
    "    lgb_params = {\n",
    "        'objective': 'regression_l1',\n",
    "        'metric': 'mae',\n",
    "        'random_state': CONFIG['random_state'],\n",
    "        'n_estimators': 4000,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 31,      # Good balance for performance and memory\n",
    "        'max_bin': 63,         # Key parameter for reducing GPU memory usage\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'n_jobs': -1,\n",
    "        'device': 'gpu',       # Enforce GPU usage\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "    }\n",
    "\n",
    "    oof_preds = np.zeros(len(y))\n",
    "    oof_scores = []\n",
    "    best_smape = np.inf\n",
    "    best_fold = -1\n",
    "    model_save_path = 'best_lgbm_model.joblib'\n",
    "\n",
    "    # Training loop with fold-wise model saving\n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(X, y), total=CONFIG['n_folds'], desc=\"Training Progress\")):\n",
    "        print(f\"\\n{'='*50}\\n--- Fold {fold+1}/{CONFIG['n_folds']} ---\\n{'='*50}\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        print(f\"  Training samples: {len(train_idx)}, Validation samples: {len(val_idx)}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train model for the current fold\n",
    "        model = lgb.LGBMRegressor(**lgb_params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(200, verbose=False),\n",
    "                lgb.log_evaluation(period=500)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Predict on validation set and calculate score\n",
    "        val_preds = model.predict(X_val)\n",
    "        oof_preds[val_idx] = val_preds\n",
    "        fold_smape = smape(y_val, val_preds)\n",
    "        oof_scores.append(fold_smape)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n  ✓ Fold {fold+1} completed in {elapsed:.1f}s\")\n",
    "        print(f\"  📊 Fold SMAPE: {fold_smape:.4f}%\")\n",
    "        \n",
    "        # --- MODEL SAVING LOGIC ---\n",
    "        # If the current model is the best one so far, save (or overwrite) it\n",
    "        if fold_smape < best_smape:\n",
    "            best_smape = fold_smape\n",
    "            best_fold = fold + 1\n",
    "            joblib.dump(model, model_save_path)\n",
    "            print(f\"  💾 New best model saved! (SMAPE: {best_smape:.4f}%)\")\n",
    "        else:\n",
    "            print(f\"  ⏭️  Model not saved (current best is {best_smape:.4f}% from fold {best_fold})\")\n",
    "        \n",
    "        print(f\"  📈 Running Mean SMAPE: {np.mean(oof_scores):.4f}% (±{np.std(oof_scores):.4f}%)\")\n",
    "    \n",
    "    # --- Final Evaluation ---\n",
    "    print(\"\\n\" + \"=\"*50); print(\"📊 Final Cross-Validation Results\"); print(\"=\"*50)\n",
    "    print(f\"Mean SMAPE across {CONFIG['n_folds']} folds: {np.mean(oof_scores):.4f}%\")\n",
    "    print(f\"Std Dev of SMAPE:  {np.std(oof_scores):.4f}%\")\n",
    "    overall_oof_smape = smape(y, oof_preds)\n",
    "    print(f\"\\nOverall OOF SMAPE (all predictions): {overall_oof_smape:.4f}%\")\n",
    "    print(f\"\\n🏆 Best single model was from Fold {best_fold} with SMAPE: {best_smape:.4f}%\")\n",
    "    print(f\"   -> This model is saved at: '{model_save_path}'\")\n",
    "    \n",
    "    # --- Retrain Final Model on Full Data ---\n",
    "    print(\"\\n\" + \"=\"*50); print(\"🔄 Retraining on 100% of data for submission...\"); print(\"=\"*50)\n",
    "    final_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    final_model.fit(X, y)\n",
    "    final_model_path = 'final_lgbm_model_submission.joblib'\n",
    "    joblib.dump(final_model, final_model_path)\n",
    "    print(f\"\\n✅ Final model for submission trained and saved as '{final_model_path}'\")\n",
    "    \n",
    "    return oof_scores\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == '__main__':\n",
    "    def load_precomputed_data(data_dir):\n",
    "        \"\"\"Loads the final feature matrix and labels.\"\"\"\n",
    "        try:\n",
    "            print(\"\\n--- Loading final precomputed feature matrix for training ---\")\n",
    "            X = load_npz(os.path.join(data_dir, 'X_final_train.npz'))\n",
    "            y_df = pd.read_csv(os.path.join(data_dir, 'y_train.csv'))\n",
    "            print(f\"   ✓ Features loaded. Shape: {X.shape}\")\n",
    "            print(f\"   ✓ Labels loaded. Count: {len(y_df)}\")\n",
    "            return X, y_df['price_log'].values\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"❌ FATAL ERROR: Could not load precomputed files. {e}\")\n",
    "            print(\"Please run the preprocessing script first to generate 'X_final_train.npz' and 'y_train.csv'.\")\n",
    "            return None, None\n",
    "\n",
    "    # Load the data generated by the optimized preprocessing script\n",
    "    X_final_train, y_final_train = load_precomputed_data(CONFIG['data_dir'])\n",
    "    \n",
    "    if X_final_train is not None:\n",
    "        oof_scores = train_lgbm_with_checkpoints(X_final_train, y_final_train)\n",
    "        print(\"\\n\" + \"=\"*50); print(\"🎉 Training Pipeline Complete!\"); print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d4a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshit\\anaconda3\\envs\\tf\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Verifying GPU setup...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 350\n",
      "[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 10\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) RaptorLake-S Mobile Graphics Controller, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 10 dense feature groups (0.00 MB) transferred to GPU in 0.001648 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.559410\n",
      "  ✅ GPU test PASSED!\n",
      "\n",
      "--- Loading final precomputed feature matrix for training ---\n",
      "   ✓ Features loaded. Shape: (75000, 9370)\n",
      "   ✓ Labels loaded. Count: 75000\n",
      "\n",
      "==================================================\n",
      "🚀 Starting LightGBM GPU Training (20 Folds + Memory Safe) 🚀\n",
      "   Folds: 20, Device: GPU\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall CV Progress:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/20 ---\n",
      "  - Data shapes: Train=(71250, 9370), Val=(3750, 9370)\n",
      "  - Sparsity: 69.03%\n",
      "  - Starting GPU training...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 1654600\n",
      "[LightGBM] [Info] Number of data points in the train set: 71250, number of used features: 9299\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) RaptorLake-S Mobile Graphics Controller, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2682 dense feature groups (182.38 MB) transferred to GPU in 0.072243 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 2.708050\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\tvalid_0's l1: 0.704925\n",
      "[100]\tvalid_0's l1: 0.659465\n",
      "[150]\tvalid_0's l1: 0.631268\n",
      "[200]\tvalid_0's l1: 0.612141\n",
      "[250]\tvalid_0's l1: 0.598433\n",
      "[300]\tvalid_0's l1: 0.587974\n",
      "[350]\tvalid_0's l1: 0.579949\n",
      "[400]\tvalid_0's l1: 0.573094\n",
      "[450]\tvalid_0's l1: 0.567664\n",
      "[500]\tvalid_0's l1: 0.562636\n",
      "[550]\tvalid_0's l1: 0.558289\n",
      "[600]\tvalid_0's l1: 0.554762\n",
      "[650]\tvalid_0's l1: 0.551564\n",
      "[700]\tvalid_0's l1: 0.548885\n",
      "[750]\tvalid_0's l1: 0.54659\n",
      "[800]\tvalid_0's l1: 0.544309\n",
      "[850]\tvalid_0's l1: 0.542278\n",
      "[900]\tvalid_0's l1: 0.540328\n",
      "[950]\tvalid_0's l1: 0.538752\n",
      "[1000]\tvalid_0's l1: 0.537445\n",
      "[1050]\tvalid_0's l1: 0.536322\n",
      "[1100]\tvalid_0's l1: 0.535388\n",
      "[1150]\tvalid_0's l1: 0.534423\n",
      "[1200]\tvalid_0's l1: 0.533627\n",
      "[1250]\tvalid_0's l1: 0.533021\n",
      "[1300]\tvalid_0's l1: 0.532364\n",
      "[1350]\tvalid_0's l1: 0.531889\n",
      "[1400]\tvalid_0's l1: 0.531431\n",
      "[1450]\tvalid_0's l1: 0.530804\n",
      "[1500]\tvalid_0's l1: 0.530254\n",
      "[1550]\tvalid_0's l1: 0.52982\n",
      "[1600]\tvalid_0's l1: 0.529304\n",
      "[1650]\tvalid_0's l1: 0.528898\n",
      "[1700]\tvalid_0's l1: 0.528577\n",
      "[1750]\tvalid_0's l1: 0.528242\n",
      "[1800]\tvalid_0's l1: 0.52796\n",
      "[1850]\tvalid_0's l1: 0.527624\n",
      "[1900]\tvalid_0's l1: 0.52734\n",
      "[1950]\tvalid_0's l1: 0.527088\n",
      "[2000]\tvalid_0's l1: 0.526813\n",
      "[2050]\tvalid_0's l1: 0.526541\n",
      "[2100]\tvalid_0's l1: 0.526254\n",
      "[2150]\tvalid_0's l1: 0.525938\n",
      "[2200]\tvalid_0's l1: 0.525688\n",
      "[2250]\tvalid_0's l1: 0.525462\n",
      "[2300]\tvalid_0's l1: 0.525188\n",
      "[2350]\tvalid_0's l1: 0.52501\n",
      "[2400]\tvalid_0's l1: 0.524764\n",
      "[2450]\tvalid_0's l1: 0.524515\n",
      "[2500]\tvalid_0's l1: 0.524321\n",
      "[2550]\tvalid_0's l1: 0.524105\n",
      "[2600]\tvalid_0's l1: 0.523899\n",
      "[2650]\tvalid_0's l1: 0.523699\n",
      "[2700]\tvalid_0's l1: 0.523561\n",
      "[2750]\tvalid_0's l1: 0.523379\n",
      "[2800]\tvalid_0's l1: 0.52317\n",
      "[2850]\tvalid_0's l1: 0.522976\n",
      "[2900]\tvalid_0's l1: 0.522832\n",
      "[2950]\tvalid_0's l1: 0.522643\n",
      "[3000]\tvalid_0's l1: 0.522479\n",
      "[3050]\tvalid_0's l1: 0.522301\n",
      "[3100]\tvalid_0's l1: 0.522123\n",
      "[3150]\tvalid_0's l1: 0.521968\n",
      "[3200]\tvalid_0's l1: 0.52185\n",
      "[3250]\tvalid_0's l1: 0.521719\n",
      "[3300]\tvalid_0's l1: 0.521616\n",
      "[3350]\tvalid_0's l1: 0.521533\n",
      "[3400]\tvalid_0's l1: 0.521412\n",
      "[3450]\tvalid_0's l1: 0.521295\n",
      "[3500]\tvalid_0's l1: 0.521189\n",
      "[3550]\tvalid_0's l1: 0.521149\n",
      "[3600]\tvalid_0's l1: 0.521064\n",
      "[3650]\tvalid_0's l1: 0.521005\n",
      "[3700]\tvalid_0's l1: 0.520921\n",
      "[3750]\tvalid_0's l1: 0.52086\n",
      "[3800]\tvalid_0's l1: 0.520792\n",
      "[3850]\tvalid_0's l1: 0.520696\n",
      "[3900]\tvalid_0's l1: 0.520606\n",
      "[3950]\tvalid_0's l1: 0.520526\n",
      "[4000]\tvalid_0's l1: 0.520433\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4000]\tvalid_0's l1: 0.520433\n",
      "    ✓ Done in 4155.2s. Fold SMAPE: 52.0306%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall CV Progress:   5%|▌         | 1/20 [1:09:19<21:57:16, 4159.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 2/20 ---\n",
      "  - Data shapes: Train=(71250, 9370), Val=(3750, 9370)\n",
      "  - Sparsity: 69.03%\n",
      "  - Starting GPU training...\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 1655881\n",
      "[LightGBM] [Info] Number of data points in the train set: 71250, number of used features: 9299\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) RaptorLake-S Mobile Graphics Controller, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2681 dense feature groups (182.38 MB) transferred to GPU in 0.268200 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 2.709383\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\tvalid_0's l1: 0.689993\n",
      "[100]\tvalid_0's l1: 0.64664\n",
      "[150]\tvalid_0's l1: 0.619046\n",
      "[200]\tvalid_0's l1: 0.600307\n",
      "[250]\tvalid_0's l1: 0.587675\n",
      "[300]\tvalid_0's l1: 0.578155\n",
      "[350]\tvalid_0's l1: 0.570684\n",
      "[400]\tvalid_0's l1: 0.565367\n",
      "[450]\tvalid_0's l1: 0.560726\n",
      "[500]\tvalid_0's l1: 0.556695\n",
      "[550]\tvalid_0's l1: 0.553167\n",
      "[600]\tvalid_0's l1: 0.550137\n",
      "[650]\tvalid_0's l1: 0.547513\n",
      "[700]\tvalid_0's l1: 0.545131\n",
      "[750]\tvalid_0's l1: 0.542951\n",
      "[800]\tvalid_0's l1: 0.541123\n",
      "[850]\tvalid_0's l1: 0.539654\n",
      "[900]\tvalid_0's l1: 0.538064\n",
      "[950]\tvalid_0's l1: 0.536713\n",
      "[1000]\tvalid_0's l1: 0.535692\n",
      "[1050]\tvalid_0's l1: 0.534738\n",
      "[1100]\tvalid_0's l1: 0.533816\n",
      "[1150]\tvalid_0's l1: 0.533102\n",
      "[1200]\tvalid_0's l1: 0.53247\n",
      "[1250]\tvalid_0's l1: 0.531782\n",
      "[1300]\tvalid_0's l1: 0.53115\n",
      "[1350]\tvalid_0's l1: 0.530567\n",
      "[1400]\tvalid_0's l1: 0.530047\n",
      "[1450]\tvalid_0's l1: 0.529532\n",
      "[1500]\tvalid_0's l1: 0.529129\n",
      "[1550]\tvalid_0's l1: 0.528786\n",
      "[1600]\tvalid_0's l1: 0.528404\n",
      "[1650]\tvalid_0's l1: 0.527976\n",
      "[1700]\tvalid_0's l1: 0.527555\n",
      "[1750]\tvalid_0's l1: 0.527139\n",
      "[1800]\tvalid_0's l1: 0.526854\n",
      "[1850]\tvalid_0's l1: 0.526594\n",
      "[1900]\tvalid_0's l1: 0.526286\n",
      "[1950]\tvalid_0's l1: 0.52597\n",
      "[2000]\tvalid_0's l1: 0.525665\n",
      "[2050]\tvalid_0's l1: 0.525338\n",
      "[2100]\tvalid_0's l1: 0.525111\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz, csr_matrix\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse\n",
    "import gc\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Configuration & Setup ---\n",
    "CONFIG = {\n",
    "    'n_folds': 20,\n",
    "    'random_state': 42,\n",
    "    'data_dir': 'processed_data',\n",
    "    'gpu_verbose': 1,\n",
    "    'batch_size': None,  # Set to int (e.g., 100_000) if you want to batch predictions\n",
    "}\n",
    "\n",
    "# --- 2. Helper Functions ---\n",
    "def smape(y_true, y_pred):\n",
    "    y_true_actual = np.expm1(y_true)\n",
    "    y_pred_actual = np.expm1(y_pred)\n",
    "    numerator = np.abs(y_pred_actual - y_true_actual)\n",
    "    denominator = (np.abs(y_true_actual) + np.abs(y_pred_actual)) / 2\n",
    "    ratio = np.where(denominator == 0, 0, numerator / denominator)\n",
    "    return np.mean(ratio) * 100\n",
    "\n",
    "def batch_predict(model, X, batch_size=100_000):\n",
    "    \"\"\"Predict in batches to reduce memory pressure.\"\"\"\n",
    "    if batch_size is None or X.shape[0] <= batch_size:\n",
    "        return model.predict(X)\n",
    "    preds = []\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        batch = X[i:i + batch_size]\n",
    "        pred = model.predict(batch)\n",
    "        preds.append(pred)\n",
    "        del batch, pred\n",
    "        gc.collect()\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "# --- 3. GPU-Optimized Training Pipeline with Memory Cleanup ---\n",
    "def train_lightgbm_gpu(X, y):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🚀 Starting LightGBM GPU Training (20 Folds + Memory Safe) 🚀\")\n",
    "    print(f\"   Folds: {CONFIG['n_folds']}, Device: GPU\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if not scipy.sparse.isspmatrix_csr(X):\n",
    "        print(\"⚠️ Converting to CSR sparse format for GPU compatibility...\")\n",
    "        X = csr_matrix(X)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['random_state'])\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression_l1',\n",
    "        'metric': 'mae',\n",
    "        'random_state': CONFIG['random_state'],\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 50,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'verbose': CONFIG['gpu_verbose'],\n",
    "    }\n",
    "\n",
    "    oof_preds = np.zeros(len(y))\n",
    "    oof_scores = []\n",
    "    models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(kf.split(X, y), total=CONFIG['n_folds'], desc=\"Overall CV Progress\")):\n",
    "        print(f\"\\n--- Fold {fold+1}/{CONFIG['n_folds']} ---\")\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        if not scipy.sparse.isspmatrix_csr(X_train):\n",
    "            X_train = csr_matrix(X_train)\n",
    "        if not scipy.sparse.isspmatrix_csr(X_val):\n",
    "            X_val = csr_matrix(X_val)\n",
    "\n",
    "        print(f\"  - Data shapes: Train={X_train.shape}, Val={X_val.shape}\")\n",
    "        print(f\"  - Sparsity: {100 * (1 - X_train.nnz / np.prod(X_train.shape)):.2f}%\")\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train, free_raw_data=True)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data, free_raw_data=True)\n",
    "\n",
    "        print(\"  - Starting GPU training...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                valid_sets=[val_data],\n",
    "                num_boost_round=4000,\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=200, verbose=True),\n",
    "                    lgb.log_evaluation(period=50)\n",
    "                ]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"❌ GPU Training FAILED! Error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Predict in batches if configured\n",
    "        val_preds = batch_predict(model, X_val, batch_size=CONFIG['batch_size'])\n",
    "        oof_preds[val_idx] = val_preds\n",
    "        fold_smape = smape(y_val, val_preds)\n",
    "        oof_scores.append(fold_smape)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"    ✓ Done in {elapsed:.1f}s. Fold SMAPE: {fold_smape:.4f}%\")\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "        # --- Aggressive memory cleanup ---\n",
    "        del X_train, X_val, y_train, y_val, train_data, val_data, val_preds\n",
    "        gc.collect()\n",
    "\n",
    "    # --- Final Evaluation ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📊 Overall Cross-Validation Results (OOF)\")\n",
    "    print(\"=\"*50)\n",
    "    mean_smape = np.mean(oof_scores)\n",
    "    std_smape = np.std(oof_scores)\n",
    "    print(f\"LGBM | Mean SMAPE: {mean_smape:.4f}% (Std: {std_smape:.4f})\")\n",
    "\n",
    "    overall_oof_smape = smape(y, oof_preds)\n",
    "    print(f\"\\nFINAL | Overall OOF SMAPE: {overall_oof_smape:.4f}%\")\n",
    "\n",
    "    # --- Retrain Final Model on Full Data ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Retraining final LightGBM model on 100% of data for submission...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if not scipy.sparse.isspmatrix_csr(X):\n",
    "        X = csr_matrix(X)\n",
    "\n",
    "    train_data_full = lgb.Dataset(X, label=y, free_raw_data=True)\n",
    "    final_model = lgb.train(\n",
    "        params,\n",
    "        train_data_full,\n",
    "        num_boost_round=4000,\n",
    "        callbacks=[lgb.log_evaluation(period=100)]\n",
    "    )\n",
    "\n",
    "    final_model.save_model('final_lgbm_gpu_model.txt')\n",
    "    print(\"\\n✅ Final retrained model saved as 'final_lgbm_gpu_model.txt'\")\n",
    "\n",
    "    # Clean up final dataset\n",
    "    del train_data_full\n",
    "    gc.collect()\n",
    "\n",
    "    return models\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"🔍 Verifying GPU setup...\")\n",
    "    try:\n",
    "        test_data = np.random.rand(100, 10)\n",
    "        test_label = np.random.rand(100)\n",
    "        test_dataset = lgb.Dataset(test_data, label=test_label)\n",
    "        test_params = {\n",
    "            'device': 'gpu',\n",
    "            'verbose': 1,\n",
    "            'num_leaves': 4,\n",
    "            'min_data_in_leaf': 1,\n",
    "            'num_iterations': 2\n",
    "        }\n",
    "        lgb.train(test_params, test_dataset)\n",
    "        print(\"  ✅ GPU test PASSED!\")\n",
    "        del test_data, test_label, test_dataset\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ GPU test FAILED! Error: {str(e)}\")\n",
    "        exit(1)\n",
    "\n",
    "    def load_precomputed_data(data_dir):\n",
    "        try:\n",
    "            print(\"\\n--- Loading final precomputed feature matrix for training ---\")\n",
    "            X = load_npz(os.path.join(data_dir, 'X_final_train.npz'))\n",
    "            y_df = pd.read_csv(os.path.join(data_dir, 'y_train.csv'))\n",
    "            print(f\"   ✓ Features loaded. Shape: {X.shape}\")\n",
    "            print(f\"   ✓ Labels loaded. Count: {len(y_df)}\")\n",
    "            return X, y_df['price_log'].values\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"FATAL ERROR: Could not load precomputed files. {e}\")\n",
    "            return None, None\n",
    "\n",
    "    X_final_train, y_final_train = load_precomputed_data(CONFIG['data_dir'])\n",
    "\n",
    "    if X_final_train is not None:\n",
    "        models = train_lightgbm_gpu(X_final_train, y_final_train)\n",
    "        # Optional: delete models if not needed to free memory\n",
    "        del models\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87229b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"--- All-in-One Prediction Script Initialized ---\")\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "# This section defines where to find all the necessary files.\n",
    "class CFG:\n",
    "    PROCESSED_DIR = 'processed_data'\n",
    "    RAW_DATA_DIR = 'dataset'\n",
    "    \n",
    "    # --- INPUT FILES ---\n",
    "    # The raw test data you want to predict on\n",
    "    TEST_CSV = os.path.join(RAW_DATA_DIR, 'test.csv')\n",
    "    \n",
    "    # The final model you trained\n",
    "    MODEL_FILE = 'final_lgbm_gpu_model.txt'\n",
    "    \n",
    "    # All the helper files and transformers saved during training\n",
    "    TEXT_PREPROCESSOR = os.path.join(PROCESSED_DIR, 'preprocessor.joblib')\n",
    "    SENTENCE_PCA_TRANSFORMER = os.path.join(PROCESSED_DIR, 'sentence_pca_transformer.joblib')\n",
    "    IMAGE_PCA_TRANSFORMER = os.path.join(PROCESSED_DIR, 'image_pca_transformer.joblib')\n",
    "    \n",
    "    # Supporting data files needed for feature creation\n",
    "    VLM_FEATURES_CSV = os.path.join(PROCESSED_DIR, 'smolvlm_extracted_features.csv')\n",
    "    IMAGE_FEATURES_NPY = os.path.join(PROCESSED_DIR, 'X_image_features.npy')\n",
    "    IMAGE_IDS_CSV = os.path.join(PROCESSED_DIR, 'all_sample_ids.csv')\n",
    "    \n",
    "    # --- OUTPUT FILE ---\n",
    "    SUBMISSION_FILE = 'submission.csv'\n",
    "    \n",
    "    # --- MODEL CONFIG (must match training) ---\n",
    "    SENTENCE_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==========================================\n",
    "if __name__ == '__main__':\n",
    "    # --- 1. Load All Necessary Models and Data ---\n",
    "    print(\"\\n--- Phase 1: Loading all models, transformers, and data ---\")\n",
    "    try:\n",
    "        df_test = pd.read_csv(CFG.TEST_CSV)\n",
    "        print(f\"  ✓ Loaded '{CFG.TEST_CSV}'\")\n",
    "        \n",
    "        model = lgb.Booster(model_file=CFG.MODEL_FILE)\n",
    "        print(f\"  ✓ Loaded trained model '{CFG.MODEL_FILE}'\")\n",
    "\n",
    "        text_preprocessor = joblib.load(CFG.TEXT_PREPROCESSOR)\n",
    "        print(f\"  ✓ Loaded text preprocessor '{CFG.TEXT_PREPROCESSOR}'\")\n",
    "        \n",
    "        pca_text = joblib.load(CFG.SENTENCE_PCA_TRANSFORMER)\n",
    "        print(f\"  ✓ Loaded sentence PCA transformer '{CFG.SENTENCE_PCA_TRANSFORMER}'\")\n",
    "        \n",
    "        pca_image = joblib.load(CFG.IMAGE_PCA_TRANSFORMER)\n",
    "        print(f\"  ✓ Loaded image PCA transformer '{CFG.IMAGE_PCA_TRANSFORMER}'\")\n",
    "\n",
    "        df_vlm = pd.read_csv(CFG.VLM_FEATURES_CSV)\n",
    "        df_image_ids = pd.read_csv(CFG.IMAGE_IDS_CSV)\n",
    "        X_image_full_mmap = np.load(CFG.IMAGE_FEATURES_NPY, mmap_mode='r')\n",
    "        print(\"  ✓ Loaded all supporting data files.\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n❌ FATAL ERROR: A required file was not found.\")\n",
    "        print(f\"   Missing file: {e.filename}\")\n",
    "        print(\"   Please ensure all training artifacts and data files are in the correct directories.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Re-create Test Features IN MEMORY ---\n",
    "    print(\"\\n--- Phase 2: Preprocessing test data in memory ---\")\n",
    "    \n",
    "    # Step A: Generate Base TF-IDF Features\n",
    "    print(\"  - Step 2A: Generating base TF-IDF features...\")\n",
    "    X_text_base_tfidf = text_preprocessor.transform(df_test)\n",
    "\n",
    "    # Step B: Generate Semantic Text Embeddings\n",
    "    print(\"  - Step 2B: Generating semantic text embeddings...\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    sentence_model = SentenceTransformer(CFG.SENTENCE_MODEL_NAME, device=device)\n",
    "    df_test_text = pd.merge(df_test[['sample_id', 'catalog_content']], df_vlm, on='sample_id', how='left')\n",
    "    df_test_text.fillna('', inplace=True)\n",
    "    corpus = (df_test_text['catalog_content'] + \" \" + df_test_text['smolvlm_description']).tolist()\n",
    "    sentence_embeddings = sentence_model.encode(corpus, show_progress_bar=True, device=device)\n",
    "    X_sentence_pca = pca_text.transform(sentence_embeddings)\n",
    "\n",
    "    # Step C: Align and Reduce Image Features\n",
    "    print(\"  - Step 2C: Aligning and reducing image features...\")\n",
    "    id_to_row_idx = pd.Series(index=df_image_ids['sample_id'], data=np.arange(len(df_image_ids)))\n",
    "    test_indices = id_to_row_idx.loc[df_test['sample_id']].values\n",
    "    X_image_test_aligned = X_image_full_mmap[test_indices, :]\n",
    "    X_image_pca = pca_image.transform(X_image_test_aligned)\n",
    "\n",
    "    # Step D: Combine all features into a single in-memory matrix\n",
    "    print(\"  - Step 2D: Combining all features into a final matrix...\")\n",
    "    X_final_test = hstack([\n",
    "        X_text_base_tfidf,\n",
    "        X_sentence_pca,\n",
    "        X_image_pca\n",
    "    ]).tocsr()\n",
    "    X_final_test.data = X_final_test.data.astype(np.float32)\n",
    "    print(f\"  ✓ Preprocessing complete. Final in-memory feature shape: {X_final_test.shape}\")\n",
    "\n",
    "    # --- 3. Generate Predictions ---\n",
    "    print(\"\\n--- Phase 3: Generating predictions ---\")\n",
    "    log_predictions = model.predict(X_final_test)\n",
    "    print(\"  ✓ Predictions generated.\")\n",
    "\n",
    "    # --- 4. Create and Save Submission File ---\n",
    "    print(\"\\n--- Phase 4: Creating submission file ---\")\n",
    "    final_prices = np.expm1(log_predictions)\n",
    "    final_prices[final_prices < 0] = 0 # Enforce non-negative price constraint\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        'sample_id': df_test['sample_id'],\n",
    "        'price': final_prices\n",
    "    })\n",
    "\n",
    "    submission_df.to_csv(CFG.SUBMISSION_FILE, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"🎉 Success! Submission file created at: '{CFG.SUBMISSION_FILE}'\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nFirst 5 rows of your submission file:\")\n",
    "    print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26818e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshit\\anaconda3\\envs\\tf\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Prediction Script (Using Base Preprocessor Only) ---\n",
      "\n",
      "--- Phase 1: Loading model and the single preprocessor ---\n",
      "  ✓ All required files loaded successfully.\n",
      "\n",
      "--- Phase 2: Preprocessing test data to match model's expectation ---\n",
      "  ✓ Preprocessing complete. Final feature shape: (75000, 5054)\n",
      "  ✓ Feature count matches the trained model. Ready to predict.\n",
      "\n",
      "--- Phase 3: Generating predictions ---\n",
      "  ✓ Predictions generated.\n",
      "\n",
      "--- Phase 4: Creating submission file ---\n",
      "\n",
      "==================================================\n",
      "🎉 Success! Submission file created at: 'submission.csv'\n",
      "==================================================\n",
      "\n",
      "First 5 rows of your submission file:\n",
      "   sample_id      price\n",
      "0     100179  10.591078\n",
      "1     245611   9.450141\n",
      "2     146263  17.000130\n",
      "3      95658   7.454436\n",
      "4      36806  16.120088\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"--- Final Prediction Script (Using Base Preprocessor Only) ---\")\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "class CFG:\n",
    "    PROCESSED_DIR = 'processed_data'\n",
    "    RAW_DATA_DIR = 'dataset'\n",
    "    \n",
    "    # --- INPUT FILES ---\n",
    "    TEST_CSV = os.path.join(RAW_DATA_DIR, 'test.csv')\n",
    "    MODEL_FILE = 'best_cpu_lgbm_model.joblib'\n",
    "    \n",
    "    # The ONLY transformer your model was trained on\n",
    "    TEXT_PREPROCESSOR = os.path.join(PROCESSED_DIR, 'preprocessor.joblib')\n",
    "    \n",
    "    # --- OUTPUT FILE ---\n",
    "    SUBMISSION_FILE = 'submission.csv'\n",
    "\n",
    "# ==========================================\n",
    "# HELPER FUNCTIONS (must match your training preprocessing)\n",
    "# ==========================================\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_structured_data(text):\n",
    "    if not isinstance(text, str): return pd.Series([np.nan, 'unknown'], index=['Value', 'Unit'])\n",
    "    value_match = re.search(r'Value:\\s*([\\d\\.]+)', text, re.IGNORECASE); value = float(value_match.group(1)) if value_match else np.nan\n",
    "    unit_match = re.search(r'Unit:\\s*(\\w+)', text, re.IGNORECASE); unit = unit_match.group(1) if unit_match else 'unknown'\n",
    "    return pd.Series([value, unit], index=['Value', 'Unit'])\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'Value:.*|Unit:.*|<.*?>|[^a-zA-Z\\s]', ' ', text, flags=re.IGNORECASE).lower()\n",
    "    words = text.split()\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in words if w not in stop_words and len(w) > 1]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "def standardize_units(unit):\n",
    "    if not isinstance(unit, str): return 'unknown'\n",
    "    unit = unit.lower().strip(); unit_map = {'oz': 'ounce', 'ounces': 'ounce', 'fl oz': 'fl_oz', 'fz': 'fl_oz', 'ct': 'count', 'none': 'unknown'}\n",
    "    return unit_map.get(unit, unit)\n",
    "\n",
    "# ==========================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==========================================\n",
    "if __name__ == '__main__':\n",
    "    # --- 1. Load Model and Preprocessor ---\n",
    "    print(\"\\n--- Phase 1: Loading model and the single preprocessor ---\")\n",
    "    try:\n",
    "        df_test = pd.read_csv(CFG.TEST_CSV)\n",
    "        model = joblib.load(CFG.MODEL_FILE)\n",
    "        preprocessor = joblib.load(CFG.TEXT_PREPROCESSOR)\n",
    "        print(\"  ✓ All required files loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n❌ FATAL ERROR: A required file was not found: {e.filename}\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Create Test Features using ONLY the Base Preprocessor ---\n",
    "    print(\"\\n--- Phase 2: Preprocessing test data to match model's expectation ---\")\n",
    "    \n",
    "    extracted_data = df_test['catalog_content'].apply(extract_structured_data)\n",
    "    features_df = pd.concat([df_test.reset_index(drop=True), extracted_data], axis=1)\n",
    "    features_df['cleaned_catalog'] = features_df['catalog_content'].apply(clean_text)\n",
    "    features_df['Unit_standardized'] = features_df['Unit'].apply(standardize_units)\n",
    "    \n",
    "    # This is the only transformation that was used for the model you have\n",
    "    X_final_test = preprocessor.transform(features_df)\n",
    "    \n",
    "    print(f\"  ✓ Preprocessing complete. Final feature shape: {X_final_test.shape}\")\n",
    "    \n",
    "    # Final sanity check\n",
    "    if X_final_test.shape[1] != model.n_features_:\n",
    "        print(f\"\\n❌ FATAL ERROR: Feature mismatch even with the simplest approach.\")\n",
    "        print(f\"   Generated {X_final_test.shape[1]} features, but model expects {model.n_features_}.\")\n",
    "        print(\"   This may mean the `preprocessor.joblib` file is from a different experiment.\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"  ✓ Feature count matches the trained model. Ready to predict.\")\n",
    "\n",
    "    # --- 3. Generate Predictions ---\n",
    "    print(\"\\n--- Phase 3: Generating predictions ---\")\n",
    "    log_predictions = model.predict(X_final_test)\n",
    "    print(\"  ✓ Predictions generated.\")\n",
    "\n",
    "    # --- 4. Create Submission File ---\n",
    "    print(\"\\n--- Phase 4: Creating submission file ---\")\n",
    "    final_prices = np.expm1(log_predictions)\n",
    "    final_prices[final_prices < 0] = 0\n",
    "\n",
    "    submission_df = pd.DataFrame({'sample_id': df_test['sample_id'], 'price': final_prices})\n",
    "    submission_df.to_csv(CFG.SUBMISSION_FILE, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"🎉 Success! Submission file created at: '{CFG.SUBMISSION_FILE}'\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nFirst 5 rows of your submission file:\")\n",
    "    print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25781fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
